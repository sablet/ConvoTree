#!/usr/bin/env python3
"""
ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 

æŠ½å‡ºã•ã‚ŒãŸã‚¤ãƒ³ãƒ†ãƒ³ãƒˆã«å¯¾ã—ã¦ã€åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®æ„å‘³çš„è·é›¢ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆéšå±¤ãƒ»æ™‚é–“ï¼‰ã‚’çµ„ã¿åˆã‚ã›ãŸ
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ

ä¸»ãªæ©Ÿèƒ½:
1. è¤‡æ•°ã®cluster_XX_processed.jsonãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
2. ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®è·é›¢ï¼ˆæ„å‘³çš„è¿‘ã•ï¼‰ã®è¨ˆç®—
3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆéšå±¤ãƒ»æ™‚é–“ï¼‰ã®æ•°å€¤åŒ–ã¨è·é›¢åŒ–
4. åˆæˆè·é›¢ã«åŸºã¥ãã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆéšå±¤çš„ãƒ»HDBSCANãƒ»k-means-constrainedï¼‰
5. ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨è©•ä¾¡æŒ‡æ¨™
"""

import json
import os
import hashlib
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import warnings
from dotenv import load_dotenv
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é–¢é€£
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
import hdbscan
from k_means_constrained import KMeansConstrained

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥
from app.cache import get_cache

# å¯è¦–åŒ–
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'Hiragino Sans'
plt.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
OUTPUT_DIR = Path("output/intent_clustering")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


@dataclass
class ClusteringConfig:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š"""
    # è·é›¢åˆæˆã®é‡ã¿ï¼ˆæ­£è¦åŒ–å¾Œï¼‰
    embedding_weight: float = 0.75  # åŸ‹ã‚è¾¼ã¿è·é›¢ã®é‡ã¿
    time_weight: float = 0.1   # æ™‚é–“è·é›¢ã®é‡ã¿
    hierarchy_weight: float = 0.15  # éšå±¤è·é›¢ã®é‡ã¿

    # æ™‚é–“ã‚«ãƒ¼ãƒãƒ«è¨­å®š
    time_bandwidth_hours: float = 168.0  # 1é€±é–“ï¼ˆæ™‚é–“å˜ä½ï¼‰

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•
    method: str = "hdbscan"  # "hdbscan", "hierarchical", or "kmeans_constrained"

    # HDBSCANãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    min_cluster_size: int = 3
    min_samples: int = 2

    # éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    n_clusters: Optional[int] = None  # Noneã®å ´åˆã¯è‡ªå‹•æ±ºå®š
    linkage: str = "average"

    # k-means-constrainedãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    size_min: int = 5  # ã‚¯ãƒ©ã‚¹ã‚¿ã®æœ€å°ã‚µã‚¤ã‚º
    size_max: int = 20  # ã‚¯ãƒ©ã‚¹ã‚¿ã®æœ€å¤§ã‚µã‚¤ã‚º
    n_init: int = 10    # k-meansã®åˆæœŸåŒ–å›æ•°
    max_iter: int = 300 # k-meansã®æœ€å¤§åå¾©å›æ•°

    # ãƒã‚¤ã‚ºå‡¦ç†
    convert_noise_to_cluster: bool = True  # ãƒã‚¤ã‚ºã‚’ã€Œãã®ä»–ã€ã‚¯ãƒ©ã‚¹ã‚¿ã¨ã—ã¦æ‰±ã†

    def validate(self):
        """è¨­å®šã®æ¤œè¨¼"""
        total_weight = self.embedding_weight + self.time_weight + self.hierarchy_weight
        if not np.isclose(total_weight, 1.0):
            raise ValueError(f"é‡ã¿ã®åˆè¨ˆãŒ1.0ã§ã‚ã‚Šã¾ã›ã‚“: {total_weight}")


class IntentData:
    """ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç®¡ç†"""

    def __init__(
        self,
        input_dir: Path,
        config: ClusteringConfig,
        model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        generate_embeddings: bool = True
    ):
        self.input_dir = Path(input_dir)
        self.config = config
        self.model_name = model_name
        self.generate_embeddings_flag = generate_embeddings

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.df = self._load_all_intents()
        self._preprocess_dataframe()

        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        self.embeddings = None
        self.embedding_dim = None
        if self.generate_embeddings_flag:
            self._generate_embeddings()

    def _load_all_intents(self) -> pd.DataFrame:
        """å…¨ã¦ã®ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        all_intents = []
        json_files = sorted(self.input_dir.glob("cluster_*_processed.json"))

        print(f"ğŸ“‚ {len(json_files)}å€‹ã®ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")

        for json_file in json_files:
            with open(json_file, 'r', encoding='utf-8') as f:
                intents = json.load(f)

            for intent_data in intents:
                # å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡ºï¼ˆæ¬ ã‘ã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
                record = {
                    'intent': intent_data.get('intent', ''),
                    'status': intent_data.get('status', 'unknown'),
                    'objective_facts': intent_data.get('objective_facts', ''),
                    'context': intent_data.get('context', ''),
                    'source_message_ids': ','.join(intent_data.get('source_message_ids', [])),
                    'original_cluster_id': intent_data.get('cluster_id', -1),
                    'source_full_paths': ','.join(intent_data.get('source_full_paths', [])),
                    'min_start_timestamp': intent_data.get('min_start_timestamp', '1970-01-01T00:00:00'),
                }
                all_intents.append(record)

        df = pd.DataFrame(all_intents)
        print(f"âœ“ {len(df)}ä»¶ã®ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        return df

    def _preprocess_dataframe(self):
        """DataFrameã®å‰å‡¦ç†"""
        # æ™‚åˆ»ã‚’datetimeã«å¤‰æ›
        self.df['start_time'] = pd.to_datetime(self.df['min_start_timestamp'])

        # ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆIDã‚’ç”Ÿæˆï¼ˆè¡Œç•ªå·ãƒ™ãƒ¼ã‚¹ï¼‰
        self.df['intent_id'] = [f"intent_{i:05d}" for i in range(len(self.df))]

        # ãƒ‘ã‚¹å‡¦ç†: source_full_pathsã‹ã‚‰æœ€åˆã®ãƒ‘ã‚¹ã‚’æŠ½å‡º
        def extract_first_path(paths_str: str) -> str:
            if pd.isna(paths_str) or paths_str == '':
                return 'Unknown'
            paths = paths_str.split(',')
            return paths[0].strip() if paths else 'Unknown'

        self.df['full_path'] = self.df['source_full_paths'].apply(extract_first_path)

        # ãƒ‘ã‚¹å‡¦ç†: Inboxã®ã¿ã®å ´åˆã¯ãã®ã¾ã¾ã€Inbox->A->Bã®å ´åˆã¯A->Bã¨ã—ã¦æ‰±ã†
        def normalize_path(path: str) -> str:
            if ' -> ' not in path:
                return path
            else:
                parts = path.split(' -> ')
                if len(parts) > 1 and parts[0] == 'Inbox':
                    return ' -> '.join(parts[1:])
                return path

        self.df['normalized_path'] = self.df['full_path'].apply(normalize_path)

        # éšå±¤æ·±ã•ã‚’è¨ˆç®—ï¼ˆæ­£è¦åŒ–ãƒ‘ã‚¹ã® ' -> ' ã®å‡ºç¾å›æ•°ï¼‰
        self.df['hierarchy_depth'] = self.df['normalized_path'].str.count(' -> ')

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥é›†è¨ˆ
        status_counts = self.df['status'].value_counts()
        print(f"  - æœŸé–“: {self.df['start_time'].min()} ã€œ {self.df['start_time'].max()}")
        print(f"  - ãƒ‘ã‚¹æ•°: {self.df['full_path'].nunique()}")
        print(f"  - æ­£è¦åŒ–ãƒ‘ã‚¹æ•°: {self.df['normalized_path'].nunique()}")
        print(f"  - æœ€å¤§éšå±¤æ·±ã•: {self.df['hierarchy_depth'].max()}")
        print(f"  - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥: {dict(status_counts)}")

    def _generate_embeddings(self):
        """åŸ‹ã‚è¾¼ã¿ã®ç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰"""
        print(f"ğŸ”„ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­ï¼ˆãƒ¢ãƒ‡ãƒ«: {self.model_name}ï¼‰...")

        cache = get_cache("intent_clustering_embeddings")
        model = SentenceTransformer(self.model_name)

        embeddings_list = []

        # é™¤å¤–ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼‰
        exclude_fields = {
            'source_message_ids', 'original_cluster_id', 'source_full_paths',
            'min_start_timestamp', 'intent_id', 'start_time', 'full_path',
            'normalized_path', 'hierarchy_depth', 'cluster'
        }

        for idx, row in tqdm(self.df.iterrows(), total=len(self.df), desc="åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"):
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã®å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’çµåˆã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
            text_parts = []
            for field, value in row.items():
                if field not in exclude_fields and pd.notna(value) and str(value).strip():
                    text_parts.append(str(value))

            text = " ".join(text_parts)
            cache_key = f"intent_embedding:{self.model_name}:{hashlib.md5(text.encode()).hexdigest()}"

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
            cached_embedding = cache.get(cache_key)
            if cached_embedding is not None:
                embedding = np.array(cached_embedding)
            else:
                # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
                embedding = model.encode(text, convert_to_numpy=True)
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                cache.set(cache_key, embedding.tolist())

            embeddings_list.append(embedding)

        self.embeddings = np.array(embeddings_list)
        self.embedding_dim = self.embeddings.shape[1]
        print(f"âœ“ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†ï¼ˆæ¬¡å…ƒ: {self.embedding_dim}ï¼‰")

    def compute_combined_distance_matrix(self) -> np.ndarray:
        """åŸ‹ã‚è¾¼ã¿ãƒ»æ™‚é–“ãƒ»éšå±¤ã‚’åˆæˆã—ãŸè·é›¢è¡Œåˆ—ã‚’è¨ˆç®—"""
        n = len(self.df)

        # 1. åŸ‹ã‚è¾¼ã¿ã®è·é›¢è¡Œåˆ—ï¼ˆã‚³ã‚µã‚¤ãƒ³è·é›¢ï¼‰
        print("ğŸ“Š åŸ‹ã‚è¾¼ã¿è·é›¢è¡Œåˆ—ã‚’è¨ˆç®—ä¸­...")
        embedding_similarity = cosine_similarity(self.embeddings)
        embedding_distance = 1 - embedding_similarity
        embedding_distance_norm = embedding_distance / (embedding_distance.max() + 1e-10)

        # 2. æ™‚é–“è·é›¢è¡Œåˆ—ï¼ˆRBFã‚«ãƒ¼ãƒãƒ«ï¼‰
        print("ğŸ“Š æ™‚é–“è·é›¢è¡Œåˆ—ã‚’è¨ˆç®—ä¸­...")
        timestamps = self.df['start_time'].values
        time_diff_matrix = np.abs(
            timestamps[:, np.newaxis] - timestamps[np.newaxis, :]
        ).astype('timedelta64[h]').astype(float)  # æ™‚é–“å˜ä½ã«å¤‰æ›

        bandwidth = self.config.time_bandwidth_hours
        time_kernel = np.exp(-0.5 * (time_diff_matrix / bandwidth) ** 2)
        time_distance = 1 - time_kernel
        time_distance_norm = time_distance / (time_distance.max() + 1e-10)

        # 3. éšå±¤è·é›¢è¡Œåˆ—ï¼ˆåŒã˜éšå±¤ãªã‚‰0ã€ç•°ãªã‚Œã°1ï¼‰
        print("ğŸ“Š éšå±¤è·é›¢è¡Œåˆ—ã‚’è¨ˆç®—ä¸­...")
        paths = self.df['normalized_path'].values
        hierarchy_distance = np.zeros((n, n))
        for i in range(n):
            for j in range(i+1, n):
                # éšå±¤ãƒ‘ã‚¹ãŒç•°ãªã‚‹å ´åˆã¯1ã€åŒã˜å ´åˆã¯0
                if paths[i] != paths[j]:
                    hierarchy_distance[i, j] = 1.0
                    hierarchy_distance[j, i] = 1.0

        hierarchy_distance_norm = hierarchy_distance

        # 4. åˆæˆè·é›¢è¡Œåˆ—
        print("ğŸ“Š åˆæˆè·é›¢è¡Œåˆ—ã‚’è¨ˆç®—ä¸­...")
        combined_distance = (
            self.config.embedding_weight * embedding_distance_norm +
            self.config.time_weight * time_distance_norm +
            self.config.hierarchy_weight * hierarchy_distance_norm
        )

        print(f"âœ“ åˆæˆè·é›¢è¡Œåˆ—å®Œæˆ (shape: {combined_distance.shape})")
        print(f"  - åŸ‹ã‚è¾¼ã¿é‡ã¿: {self.config.embedding_weight}")
        print(f"  - æ™‚é–“é‡ã¿: {self.config.time_weight}")
        print(f"  - éšå±¤é‡ã¿: {self.config.hierarchy_weight}")

        return combined_distance

    def cluster(self, distance_matrix: np.ndarray) -> np.ndarray:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        print(f"ğŸ” ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œï¼ˆæ‰‹æ³•: {self.config.method}ï¼‰...")

        if self.config.method == "hdbscan":
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=self.config.min_cluster_size,
                min_samples=self.config.min_samples,
                metric='precomputed',
                cluster_selection_method='eom'
            )
            labels = clusterer.fit_predict(distance_matrix)

        elif self.config.method == "hierarchical":
            n_clusters = self.config.n_clusters
            if n_clusters is None:
                # è‡ªå‹•æ±ºå®šï¼ˆä¾‹: âˆšnï¼‰
                n_clusters = max(2, int(np.sqrt(len(self.df))))

            clusterer = AgglomerativeClustering(
                n_clusters=n_clusters,
                metric='precomputed',
                linkage=self.config.linkage
            )
            labels = clusterer.fit_predict(distance_matrix)

        elif self.config.method == "kmeans_constrained":
            n_clusters = self.config.n_clusters
            if n_clusters is None:
                # è‡ªå‹•æ±ºå®š
                n = len(self.df)
                avg_size = (self.config.size_min + self.config.size_max) / 2
                n_clusters = max(2, int(n / avg_size))

            # è·é›¢è¡Œåˆ—ã‚’ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆMDSçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
            from sklearn.manifold import MDS
            mds = MDS(n_components=min(10, len(self.df) - 1), dissimilarity='precomputed', random_state=42)
            X_mds = mds.fit_transform(distance_matrix)

            clusterer = KMeansConstrained(
                n_clusters=n_clusters,
                size_min=self.config.size_min,
                size_max=self.config.size_max,
                n_init=self.config.n_init,
                max_iter=self.config.max_iter,
                random_state=42
            )
            labels = clusterer.fit_predict(X_mds)

        else:
            raise ValueError(f"æœªå¯¾å¿œã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•: {self.config.method}")

        # ãƒã‚¤ã‚ºï¼ˆ-1ï¼‰ã‚’ã€Œãã®ä»–ã€ã‚¯ãƒ©ã‚¹ã‚¿ã«å¤‰æ›
        if self.config.convert_noise_to_cluster and (labels == -1).any():
            max_label = labels.max()
            labels[labels == -1] = max_label + 1
            print(f"  âš ï¸ ãƒã‚¤ã‚º {(labels == max_label + 1).sum()}ä»¶ã‚’ã€Œãã®ä»–ã€ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆ{max_label + 1}ï¼‰ã«å¤‰æ›ã—ã¾ã—ãŸ")

        unique_labels = np.unique(labels)
        print(f"âœ“ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†")
        print(f"  - ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(unique_labels)}")
        print(f"  - ãƒã‚¤ã‚º: {(labels == -1).sum()}ä»¶")

        return labels

    def save_results(self, labels: np.ndarray):
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®ä¿å­˜"""
        self.df['cluster'] = labels

        # CSVå‡ºåŠ›
        output_csv = OUTPUT_DIR / "clustered_intents.csv"
        self.df.to_csv(output_csv, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ çµæœã‚’ä¿å­˜: {output_csv}")

        # ã‚¯ãƒ©ã‚¹ã‚¿çµ±è¨ˆ
        cluster_stats = self.df['cluster'].value_counts().sort_index()
        print(f"\nğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚ºçµ±è¨ˆ:")
        for cluster_id, count in cluster_stats.items():
            print(f"  - ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}: {count}ä»¶")

        stats_json = OUTPUT_DIR / "clustering_stats.json"
        with open(stats_json, 'w', encoding='utf-8') as f:
            json.dump({
                'cluster_sizes': cluster_stats.to_dict(),
                'total_intents': len(self.df),
                'n_clusters': len(cluster_stats),
                'config': {
                    'method': self.config.method,
                    'embedding_weight': self.config.embedding_weight,
                    'time_weight': self.config.time_weight,
                    'hierarchy_weight': self.config.hierarchy_weight,
                }
            }, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜: {stats_json}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import argparse

    parser = argparse.ArgumentParser(description='ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°')

    # å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    parser.add_argument('--input-dir', type=str,
                       default='output/intent_extraction/processed',
                       help='ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆæŠ½å‡ºçµæœã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')

    # é‡ã¿è¨­å®š
    parser.add_argument('--embedding-weight', type=float, default=0.7,
                       help='åŸ‹ã‚è¾¼ã¿é‡ã¿ (default: 0.7)')
    parser.add_argument('--time-weight', type=float, default=0.15,
                       help='æ™‚é–“é‡ã¿ (default: 0.15)')
    parser.add_argument('--hierarchy-weight', type=float, default=0.15,
                       help='éšå±¤é‡ã¿ (default: 0.15)')
    parser.add_argument('--time-bandwidth-hours', type=float, default=168.0,
                       help='æ™‚é–“ã‚«ãƒ¼ãƒãƒ«å¸¯åŸŸå¹…ï¼ˆæ™‚é–“ï¼‰ (default: 168.0)')

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•
    parser.add_argument('--method', type=str, default='kmeans_constrained',
                       choices=['hdbscan', 'hierarchical', 'kmeans_constrained'],
                       help='ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³• (default: kmeans_constrained)')

    # HDBSCANãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--min-cluster-size', type=int, default=3,
                       help='HDBSCANã®æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º (default: 3)')
    parser.add_argument('--min-samples', type=int, default=2,
                       help='HDBSCANã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•° (default: 2)')

    # éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°/k-meansãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--n-clusters', type=int, default=None,
                       help='ã‚¯ãƒ©ã‚¹ã‚¿æ•° (default: è‡ªå‹•è¨ˆç®—)')
    parser.add_argument('--linkage', type=str, default='complete',
                       choices=['average', 'complete', 'single', 'ward'],
                       help='éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®çµåˆæ³• (default: complete)')

    # k-means-constrainedãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--size-min', type=int, default=5,
                       help='æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º (default: 5)')
    parser.add_argument('--size-max', type=int, default=20,
                       help='æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º (default: 20)')
    parser.add_argument('--n-init', type=int, default=10,
                       help='k-meansã®åˆæœŸåŒ–å›æ•° (default: 10)')
    parser.add_argument('--max-iter', type=int, default=300,
                       help='k-meansã®æœ€å¤§åå¾©å›æ•° (default: 300)')

    args = parser.parse_args()

    # è¨­å®š
    config = ClusteringConfig(
        embedding_weight=args.embedding_weight,
        time_weight=args.time_weight,
        hierarchy_weight=args.hierarchy_weight,
        time_bandwidth_hours=args.time_bandwidth_hours,
        method=args.method,
        min_cluster_size=args.min_cluster_size,
        min_samples=args.min_samples,
        n_clusters=args.n_clusters,
        linkage=args.linkage,
        size_min=args.size_min,
        size_max=args.size_max,
        n_init=args.n_init,
        max_iter=args.max_iter,
    )
    config.validate()

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
    intent_data = IntentData(
        input_dir=Path(args.input_dir),
        config=config,
        generate_embeddings=True
    )

    # è·é›¢è¡Œåˆ—è¨ˆç®—
    distance_matrix = intent_data.compute_combined_distance_matrix()

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    labels = intent_data.cluster(distance_matrix)

    # çµæœä¿å­˜
    intent_data.save_results(labels)

    print(f"\nâœ… å®Œäº†ï¼")
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
