#!/usr/bin/env python3
"""
ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 

åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®æ„å‘³çš„è·é›¢ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒãƒ£ãƒãƒ«éšå±¤ãƒ»æ™‚é–“ï¼‰ã‚’çµ„ã¿åˆã‚ã›ãŸ
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

ä¸»ãªæ©Ÿèƒ½:
1. åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®è·é›¢ï¼ˆæ„å‘³çš„è¿‘ã•ï¼‰ã®è¨ˆç®—
2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒãƒ£ãƒãƒ«éšå±¤ãƒ»æ™‚é–“ï¼‰ã®æ•°å€¤åŒ–ã¨è·é›¢åŒ–
3. åˆæˆè·é›¢ã«åŸºã¥ãã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆéšå±¤çš„ãƒ»HDBSCANï¼‰
4. ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨è©•ä¾¡æŒ‡æ¨™
"""

import json
import hashlib
import time
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Tuple, Optional, List
from dataclasses import dataclass
import warnings
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from joblib import Memory

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é–¢é€£
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sklearn.cluster import AgglomerativeClustering
import hdbscan
from k_means_constrained import KMeansConstrained

# å¯è¦–åŒ–
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# ã‚°ãƒ©ãƒ•åˆ†æ

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'Hiragino Sans'
plt.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
OUTPUT_DIR = Path("output/message_clustering")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šï¼ˆjoblib.Memoryä½¿ç”¨ï¼‰
CACHE_DIR = Path("output/cache/message_clustering_embeddings_ruri")
memory = Memory(location=str(CACHE_DIR), verbose=0)

# ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆé…å»¶ãƒ­ãƒ¼ãƒ‰ï¼‰
_model_instance = None


def _get_model() -> SentenceTransformer:
    """
    SentenceTransformerãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ï¼ˆåˆå›ã®ã¿ãƒ­ãƒ¼ãƒ‰ï¼‰

    Returns:
        SentenceTransformerãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    global _model_instance
    if _model_instance is None:
        print("  ruri-large-v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        _model_instance = SentenceTransformer("cl-nagoya/ruri-large-v2")
        print("  âœ“ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
    return _model_instance


@memory.cache
def _compute_embeddings_cached(texts: List[str], cache_key: str) -> List[List[float]]:
    """
    åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰

    Args:
        texts: ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        cache_key: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ï¼ˆSHA256ãƒãƒƒã‚·ãƒ¥ï¼‰

    Returns:
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    print("  åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆä¸­...")
    model = _get_model()
    batch_embeddings = model.encode(
        texts,
        convert_to_tensor=False,
        show_progress_bar=True,
        batch_size=32
    )
    return [embedding.tolist() for embedding in batch_embeddings]


@dataclass
class ClusteringConfig:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š"""
    # è·é›¢åˆæˆã®é‡ã¿ï¼ˆæ­£è¦åŒ–å¾Œï¼‰
    embedding_weight: float = 0.75  # åŸ‹ã‚è¾¼ã¿è·é›¢ã®é‡ã¿
    time_weight: float = 0.1   # æ™‚é–“è·é›¢ã®é‡ã¿
    hierarchy_weight: float = 0.15  # éšå±¤è·é›¢ã®é‡ã¿

    # æ™‚é–“ã‚«ãƒ¼ãƒãƒ«è¨­å®š
    time_bandwidth_hours: float = 168.0  # 1é€±é–“ï¼ˆæ™‚é–“å˜ä½ï¼‰

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•
    method: str = "hdbscan"  # "hdbscan", "hierarchical", or "kmeans_constrained"

    # HDBSCANãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    min_cluster_size: int = 5
    min_samples: int = 3

    # éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    n_clusters: Optional[int] = None  # Noneã®å ´åˆã¯è‡ªå‹•æ±ºå®š
    linkage: str = "average"

    # k-means-constrainedãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    size_min: int = 10  # ã‚¯ãƒ©ã‚¹ã‚¿ã®æœ€å°ã‚µã‚¤ã‚º
    size_max: int = 50  # ã‚¯ãƒ©ã‚¹ã‚¿ã®æœ€å¤§ã‚µã‚¤ã‚º
    n_init: int = 10    # k-meansã®åˆæœŸåŒ–å›æ•°
    max_iter: int = 300 # k-meansã®æœ€å¤§åå¾©å›æ•°

    # ãƒã‚¤ã‚ºå‡¦ç†
    convert_noise_to_cluster: bool = True  # ãƒã‚¤ã‚ºã‚’ã€Œãã®ä»–ã€ã‚¯ãƒ©ã‚¹ã‚¿ã¨ã—ã¦æ‰±ã†

    def validate(self):
        """è¨­å®šã®æ¤œè¨¼"""
        total_weight = self.embedding_weight + self.time_weight + self.hierarchy_weight
        if not np.isclose(total_weight, 1.0):
            raise ValueError(f"é‡ã¿ã®åˆè¨ˆãŒ1.0ã§ã‚ã‚Šã¾ã›ã‚“: {total_weight}")


class MessageData:
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã®ç®¡ç†"""

    def __init__(self, csv_path: str, embedding_path: Optional[str] = None, generate_embeddings: bool = True):
        """
        Args:
            csv_path: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸CSVã®ãƒ‘ã‚¹
            embedding_path: åŸ‹ã‚è¾¼ã¿JSONã®ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆã¾ãŸã¯åŸ‹ã‚è¾¼ã¿ç„¡ã—ï¼‰
            generate_embeddings: åŸ‹ã‚è¾¼ã¿ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã‹
        """
        self.csv_path = Path(csv_path)
        self.embedding_path = Path(embedding_path) if embedding_path else None
        self.generate_embeddings_flag = generate_embeddings

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.df = pd.read_csv(self.csv_path)
        self._preprocess_dataframe()

        # åŸ‹ã‚è¾¼ã¿èª­ã¿è¾¼ã¿ã¾ãŸã¯ç”Ÿæˆ
        self.embeddings = None
        self.embedding_dim = None
        if self.embedding_path and self.embedding_path.exists():
            self._load_embeddings()
        elif self.generate_embeddings_flag:
            self._generate_embeddings()

    def _preprocess_dataframe(self):
        """DataFrameã®å‰å‡¦ç†"""
        # æ™‚åˆ»ã‚’datetimeã«å¤‰æ›
        self.df['start_time'] = pd.to_datetime(self.df['start_time'])
        self.df['end_time'] = pd.to_datetime(self.df['end_time'])

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸IDã‚’ç”Ÿæˆï¼ˆè¡Œç•ªå·ãƒ™ãƒ¼ã‚¹ï¼‰
        self.df['message_id'] = [f"msg_{i:05d}" for i in range(len(self.df))]

        # ãƒ‘ã‚¹å‡¦ç†: Inboxã®ã¿ã®å ´åˆã¯ãã®ã¾ã¾ã€Inbox->A->Bã®å ´åˆã¯A->Bã¨ã—ã¦æ‰±ã†
        def normalize_path(path: str) -> str:
            # ' -> ' ã§åˆ†å‰²ï¼ˆå®Ÿéš›ã®åŒºåˆ‡ã‚Šæ–‡å­—ï¼‰
            if ' -> ' not in path:
                # Inboxã®ã¿ã®å ´åˆã¯ãã®ã¾ã¾
                return path
            else:
                # Inbox -> A -> B ã®å ´åˆã€Inboxã‚’é™¤å»ã—ã¦A -> Bã«ã™ã‚‹
                parts = path.split(' -> ')
                if len(parts) > 1 and parts[0] == 'Inbox':
                    return ' -> '.join(parts[1:])
                return path

        self.df['normalized_path'] = self.df['full_path'].apply(normalize_path)

        # éšå±¤æ·±ã•ã‚’è¨ˆç®—ï¼ˆæ­£è¦åŒ–ãƒ‘ã‚¹ã® ' -> ' ã®å‡ºç¾å›æ•°ï¼‰
        self.df['hierarchy_depth'] = self.df['normalized_path'].str.count(' -> ')

        print(f"âœ“ {len(self.df)}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        print(f"  - æœŸé–“: {self.df['start_time'].min()} ã€œ {self.df['start_time'].max()}")
        print(f"  - ãƒãƒ£ãƒãƒ«æ•°: {self.df['full_path'].nunique()}")
        print(f"  - æ­£è¦åŒ–ãƒãƒ£ãƒãƒ«æ•°: {self.df['normalized_path'].nunique()}")
        print(f"  - æœ€å¤§éšå±¤æ·±ã•: {self.df['hierarchy_depth'].max()}")

    def _load_embeddings(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        with open(self.embedding_path, 'r', encoding='utf-8') as f:
            embedding_data = json.load(f)

        # message_idã¨embeddingã®å¯¾å¿œã‚’ä½œæˆ
        embedding_dict = {item['id']: item['embedding'] for item in embedding_data}

        # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã‚’å–å¾—
        first_embedding = embedding_data[0]['embedding']
        embedding_dim = len(first_embedding)

        # DataFrameã®é †åºã«åˆã‚ã›ã¦åŸ‹ã‚è¾¼ã¿ã‚’é…ç½®
        embeddings_list = []
        for msg_id in self.df['message_id']:
            if msg_id in embedding_dict:
                embeddings_list.append(embedding_dict[msg_id])
            else:
                # åŸ‹ã‚è¾¼ã¿ãŒç„¡ã„å ´åˆã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
                embeddings_list.append([0.0] * embedding_dim)

        self.embeddings = np.array(embeddings_list)
        self.embedding_dim = self.embeddings.shape[1]

        print(f"âœ“ åŸ‹ã‚è¾¼ã¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {self.embeddings.shape}")

    def _generate_embeddings(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆï¼ˆruri-large-v2ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰"""
        print("\nåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆä¸­...")

        # ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æº–å‚™
        texts = []
        indices = []
        for idx, row in self.df.iterrows():
            text = row['combined_content']
            if not pd.isna(text) and str(text).strip() != "":
                texts.append(str(text))
                indices.append(idx)

        # åŸ‹ã‚è¾¼ã¿ãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–ï¼ˆã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã§åŸ‹ã‚ã‚‹ï¼‰
        embeddings_list = [[0.0] * 1024 for _ in range(len(self.df))]

        print(f"  ç·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {len(self.df)}")
        print(f"  å‡¦ç†å¯¾è±¡: {len(texts)}ä»¶")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆï¼ˆå…¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥ï¼‰
        cache_key = hashlib.sha256("\n".join(texts).encode('utf-8')).hexdigest()

        # å®Ÿè¡Œæ™‚é–“è¨ˆæ¸¬é–‹å§‹
        start_time = time.time()

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãåŸ‹ã‚è¾¼ã¿è¨ˆç®—ã‚’å®Ÿè¡Œ
        batch_embeddings = _compute_embeddings_cached(texts, cache_key)

        # å®Ÿè¡Œæ™‚é–“è¨ˆæ¸¬çµ‚äº†
        elapsed_time = time.time() - start_time

        # çµæœã‚’å¯¾å¿œã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«æ ¼ç´
        for idx, embedding in zip(indices, batch_embeddings):
            embeddings_list[idx] = embedding

        self.embeddings = np.array(embeddings_list)
        self.embedding_dim = self.embeddings.shape[1]

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ/ãƒŸã‚¹ã®åˆ¤å®šï¼ˆé«˜é€Ÿãªã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼‰
        if elapsed_time < 1.0:
            print(f"  âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ï¼ˆå®Ÿè¡Œæ™‚é–“: {elapsed_time:.3f}ç§’ï¼‰")
        else:
            print(f"  âœ“ åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼ˆå®Ÿè¡Œæ™‚é–“: {elapsed_time:.2f}ç§’ï¼‰")

        print(f"âœ“ åŸ‹ã‚è¾¼ã¿å®Œäº†: {self.embeddings.shape}")

        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: JSONå½¢å¼ã§ä¿å­˜
        self._save_embeddings()

    def _save_embeddings(self):
        """åŸ‹ã‚è¾¼ã¿ã‚’JSONå½¢å¼ã§ä¿å­˜"""
        embeddings_data = []
        for i, msg_id in enumerate(self.df['message_id']):
            embeddings_data.append({
                'id': msg_id,
                'embedding': self.embeddings[i].tolist()
            })

        # å‡ºåŠ›ãƒ‘ã‚¹
        output_path = OUTPUT_DIR / "messages_embedded.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(embeddings_data, f, ensure_ascii=False, indent=2)

        print(f"  â†’ åŸ‹ã‚è¾¼ã¿ã‚’ä¿å­˜: {output_path}")

    def has_embeddings(self) -> bool:
        """åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½ã‹"""
        return self.embeddings is not None


class DistanceCalculator:
    """è·é›¢è¨ˆç®—"""

    @staticmethod
    def compute_embedding_distance(embeddings: np.ndarray) -> np.ndarray:
        """
        åŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®è·é›¢è¡Œåˆ—ã‚’è¨ˆç®—

        Args:
            embeddings: åŸ‹ã‚è¾¼ã¿è¡Œåˆ— (n_samples, embedding_dim)

        Returns:
            è·é›¢è¡Œåˆ— (n_samples, n_samples)
        """
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ â†’ è·é›¢ã«å¤‰æ›
        similarity = cosine_similarity(embeddings)
        # è·é›¢ = 1 - é¡ä¼¼åº¦ï¼ˆ0ã€œ2ã®ç¯„å›²ï¼‰
        distance = 1 - similarity
        np.fill_diagonal(distance, 0)  # è‡ªå·±è·é›¢ã‚’0ã«
        return distance

    @staticmethod
    def compute_time_distance(df: pd.DataFrame, bandwidth_hours: float) -> np.ndarray:
        """
        æ™‚é–“è·é›¢è¡Œåˆ—ã‚’è¨ˆç®—ï¼ˆã‚¬ã‚¦ã‚·ã‚¢ãƒ³ã‚«ãƒ¼ãƒãƒ«ï¼‰

        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            bandwidth_hours: æ™‚é–“ã‚«ãƒ¼ãƒãƒ«ã®å¸¯åŸŸå¹…ï¼ˆæ™‚é–“å˜ä½ï¼‰

        Returns:
            æ™‚é–“è·é›¢è¡Œåˆ— (n_samples, n_samples)
        """
        # æ™‚åˆ»ã‚’æ•°å€¤åŒ–ï¼ˆUnix timestampï¼‰
        timestamps = df['start_time'].astype(np.int64) / 1e9 / 3600  # æ™‚é–“å˜ä½
        timestamps = timestamps.values.reshape(-1, 1)

        # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚’è¨ˆç®—
        time_diff = euclidean_distances(timestamps, timestamps)

        # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ã‚«ãƒ¼ãƒãƒ«ã§è·é›¢åŒ–ï¼ˆ0ã€œ1ã®ç¯„å›²ã«æ­£è¦åŒ–ï¼‰
        # è¿‘ã„æ™‚åˆ»ã»ã©è·é›¢ãŒå°ã•ããªã‚‹
        time_distance = 1 - np.exp(-(time_diff ** 2) / (2 * bandwidth_hours ** 2))
        np.fill_diagonal(time_distance, 0)

        return time_distance

    @staticmethod
    def compute_hierarchy_distance(df: pd.DataFrame) -> np.ndarray:
        """
        éšå±¤è·é›¢è¡Œåˆ—ã‚’è¨ˆç®—ï¼ˆæ­£è¦åŒ–ãƒ‘ã‚¹ã‚’ä½¿ç”¨ï¼‰

        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame

        Returns:
            éšå±¤è·é›¢è¡Œåˆ— (n_samples, n_samples)
        """
        n = len(df)
        hierarchy_distance = np.zeros((n, n))

        # æ­£è¦åŒ–ãƒ‘ã‚¹ã‚’ä½¿ç”¨
        paths = df['normalized_path'].values

        for i in range(n):
            for j in range(i + 1, n):
                # å…±é€šãƒ‘ã‚¹é•·ã‚’è¨ˆç®—ï¼ˆ' -> ' ã§åˆ†å‰²ï¼‰
                path_i = paths[i].split(' -> ')
                path_j = paths[j].split(' -> ')

                common_depth = 0
                for pi, pj in zip(path_i, path_j):
                    if pi == pj:
                        common_depth += 1
                    else:
                        break

                # è·é›¢ = æœ€å¤§æ·±ã• - å…±é€šæ·±ã•ï¼ˆæ­£è¦åŒ–ï¼‰
                max_depth = max(len(path_i), len(path_j))
                distance = (max_depth - common_depth) / max_depth if max_depth > 0 else 0

                hierarchy_distance[i, j] = distance
                hierarchy_distance[j, i] = distance

        return hierarchy_distance

    @staticmethod
    def combine_distances(
        embedding_dist: Optional[np.ndarray],
        time_dist: np.ndarray,
        hierarchy_dist: np.ndarray,
        config: ClusteringConfig
    ) -> np.ndarray:
        """
        è¤‡æ•°ã®è·é›¢è¡Œåˆ—ã‚’æ­£è¦åŒ–ã—ã¦é‡ã¿ä»˜ã‘åˆæˆ

        å„è·é›¢ã‚’æœ€å°å€¤0ã«ã‚·ãƒ•ãƒˆã—ã¦ã‹ã‚‰æ¨™æº–åå·®ã§å‰²ã‚‹ã“ã¨ã§æ­£è¦åŒ–ã€‚
        è·é›¢ã®æ€§è³ªï¼ˆæœ€å°å€¤0ã€éè² æ€§ï¼‰ã‚’ä¿æŒã—ã¤ã¤ã€æ¨™æº–åå·®ã‚’çµ±ä¸€ã€‚

        Args:
            embedding_dist: åŸ‹ã‚è¾¼ã¿è·é›¢è¡Œåˆ—ï¼ˆNoneã®å ´åˆã¯ä½¿ç”¨ã—ãªã„ï¼‰
            time_dist: æ™‚é–“è·é›¢è¡Œåˆ—
            hierarchy_dist: éšå±¤è·é›¢è¡Œåˆ—
            config: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š

        Returns:
            åˆæˆè·é›¢è¡Œåˆ—
        """
        n = time_dist.shape[0]

        # å„è·é›¢è¡Œåˆ—ã‚’æ­£è¦åŒ–ï¼ˆä¸Šä¸‰è§’ã®ã¿ä½¿ç”¨ï¼‰
        triu_indices = np.triu_indices(n, k=1)

        # æ™‚é–“è·é›¢ã®æ­£è¦åŒ–ï¼ˆæœ€å°å€¤0ã«ã‚·ãƒ•ãƒˆã€æ¨™æº–åå·®ã§å‰²ã‚‹ï¼‰
        time_vals = time_dist[triu_indices]
        time_min = time_vals.min()
        time_shifted = time_vals - time_min
        time_std = time_shifted.std()
        if time_std > 0:
            time_normalized_vals = time_shifted / time_std
        else:
            time_normalized_vals = time_shifted
        time_normalized = np.zeros_like(time_dist)
        time_normalized[triu_indices] = time_normalized_vals
        time_normalized = time_normalized + time_normalized.T

        # éšå±¤è·é›¢ã®æ­£è¦åŒ–ï¼ˆæœ€å°å€¤0ã«ã‚·ãƒ•ãƒˆã€æ¨™æº–åå·®ã§å‰²ã‚‹ï¼‰
        hier_vals = hierarchy_dist[triu_indices]
        hier_min = hier_vals.min()
        hier_shifted = hier_vals - hier_min
        hier_std = hier_shifted.std()
        if hier_std > 0:
            hier_normalized_vals = hier_shifted / hier_std
        else:
            hier_normalized_vals = hier_shifted
        hier_normalized = np.zeros_like(hierarchy_dist)
        hier_normalized[triu_indices] = hier_normalized_vals
        hier_normalized = hier_normalized + hier_normalized.T

        if embedding_dist is None:
            # åŸ‹ã‚è¾¼ã¿ãŒç„¡ã„å ´åˆã¯æ™‚é–“ã¨éšå±¤ã®ã¿
            total_weight = config.time_weight + config.hierarchy_weight
            combined = (
                config.time_weight / total_weight * time_normalized +
                config.hierarchy_weight / total_weight * hier_normalized
            )
        else:
            # åŸ‹ã‚è¾¼ã¿è·é›¢ã®æ­£è¦åŒ–ï¼ˆæœ€å°å€¤0ã«ã‚·ãƒ•ãƒˆã€æ¨™æº–åå·®ã§å‰²ã‚‹ï¼‰
            embed_vals = embedding_dist[triu_indices]
            embed_min = embed_vals.min()
            embed_shifted = embed_vals - embed_min
            embed_std = embed_shifted.std()
            if embed_std > 0:
                embed_normalized_vals = embed_shifted / embed_std
            else:
                embed_normalized_vals = embed_shifted
            embed_normalized = np.zeros_like(embedding_dist)
            embed_normalized[triu_indices] = embed_normalized_vals
            embed_normalized = embed_normalized + embed_normalized.T

            # å…¨ã¦ã®è·é›¢ã‚’åˆæˆ
            combined = (
                config.embedding_weight * embed_normalized +
                config.time_weight * time_normalized +
                config.hierarchy_weight * hier_normalized
            )

        return combined


class ClusterAnalyzer:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œã¨åˆ†æ"""

    def __init__(self, data: MessageData, config: ClusteringConfig):
        self.data = data
        self.config = config

        # è·é›¢è¡Œåˆ—ã‚’è¨ˆç®—
        self._compute_distances()

    def _compute_distances(self):
        """å„ç¨®è·é›¢è¡Œåˆ—ã‚’è¨ˆç®—"""
        print("\nè·é›¢è¡Œåˆ—ã‚’è¨ˆç®—ä¸­...")

        calculator = DistanceCalculator()

        # åŸ‹ã‚è¾¼ã¿è·é›¢
        if self.data.has_embeddings():
            self.embedding_dist = calculator.compute_embedding_distance(self.data.embeddings)
            print(f"  âœ“ åŸ‹ã‚è¾¼ã¿è·é›¢: {self.embedding_dist.shape}")
        else:
            self.embedding_dist = None
            print("  ! åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒç„¡ã„ãŸã‚ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨")

        # æ™‚é–“è·é›¢
        self.time_dist = calculator.compute_time_distance(
            self.data.df, self.config.time_bandwidth_hours
        )
        print(f"  âœ“ æ™‚é–“è·é›¢: {self.time_dist.shape}")

        # éšå±¤è·é›¢
        self.hierarchy_dist = calculator.compute_hierarchy_distance(self.data.df)
        print(f"  âœ“ éšå±¤è·é›¢: {self.hierarchy_dist.shape}")

        # åˆæˆè·é›¢
        self.combined_dist = calculator.combine_distances(
            self.embedding_dist, self.time_dist, self.hierarchy_dist, self.config
        )
        print(f"  âœ“ åˆæˆè·é›¢: {self.combined_dist.shape}")

    def cluster(self) -> np.ndarray:
        """
        ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ

        Returns:
            ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«ï¼ˆãƒã‚¤ã‚ºã¯ã€Œãã®ä»–ã€ã‚¯ãƒ©ã‚¹ã‚¿ã¨ã—ã¦æ‰±ã†ï¼‰
        """
        print(f"\nã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­ï¼ˆæ‰‹æ³•: {self.config.method}ï¼‰...")

        if self.config.method == "hdbscan":
            clusterer = hdbscan.HDBSCAN(
                metric='precomputed',
                min_cluster_size=self.config.min_cluster_size,
                min_samples=self.config.min_samples,
                cluster_selection_method='eom'
            )
            labels = clusterer.fit_predict(self.combined_dist)

        elif self.config.method == "hierarchical":
            # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®æ±ºå®š
            n_clusters = self.config.n_clusters
            if n_clusters is None:
                # ãƒ‡ãƒ³ãƒ‰ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰è‡ªå‹•æ±ºå®šï¼ˆä»®å®Ÿè£…: sqrt(n)ï¼‰
                n_clusters = int(np.sqrt(len(self.data.df)))

            clusterer = AgglomerativeClustering(
                n_clusters=n_clusters,
                metric='precomputed',
                linkage=self.config.linkage
            )
            labels = clusterer.fit_predict(self.combined_dist)

        elif self.config.method == "kmeans_constrained":
            # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®æ±ºå®š
            n_clusters = self.config.n_clusters
            if n_clusters is None:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º/å¹³å‡ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚ºã§è‡ªå‹•æ±ºå®š
                avg_size = (self.config.size_min + self.config.size_max) / 2
                n_clusters = int(len(self.data.df) / avg_size)

            # k-means-constrainedã¯è·é›¢è¡Œåˆ—ã§ã¯ãªãç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ãŒå¿…è¦
            # è·é›¢è¡Œåˆ—ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ç©ºé–“ã‚’å†æ§‹æˆï¼ˆMDSçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
            from sklearn.manifold import MDS
            # æ¬¡å…ƒæ•°ã¯ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®2å€ç¨‹åº¦ï¼ˆçµŒé¨“å‰‡ï¼‰
            n_components = min(n_clusters * 2, len(self.data.df) - 1)
            mds = MDS(n_components=n_components, dissimilarity='precomputed', random_state=42)
            X_embedded = mds.fit_transform(self.combined_dist)

            clusterer = KMeansConstrained(
                n_clusters=n_clusters,
                size_min=self.config.size_min,
                size_max=self.config.size_max,
                n_init=self.config.n_init,
                max_iter=self.config.max_iter,
                random_state=42
            )
            labels = clusterer.fit_predict(X_embedded)

        else:
            raise ValueError(f"æœªå¯¾å¿œã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•: {self.config.method}")

        # çµ±è¨ˆæƒ…å ±
        n_clusters_orig = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)

        print(f"  âœ“ ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters_orig}")
        print(f"  âœ“ ãƒã‚¤ã‚º: {n_noise}ä»¶")

        # ãƒã‚¤ã‚ºã‚’ã€Œãã®ä»–ã€ã‚¯ãƒ©ã‚¹ã‚¿ã«å¤‰æ›
        if self.config.convert_noise_to_cluster and n_noise > 0:
            max_label = labels.max()
            others_label = max_label + 1
            labels = np.where(labels == -1, others_label, labels)
            print(f"  âœ“ ãƒã‚¤ã‚ºã‚’ã€Œãã®ä»–ã€ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆID={others_label}ï¼‰ã«å¤‰æ›")

        return labels

    def evaluate_clustering(self, labels: np.ndarray) -> Dict:
        """
        ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®è©•ä¾¡

        Args:
            labels: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«

        Returns:
            è©•ä¾¡æŒ‡æ¨™ã®è¾æ›¸
        """
        from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

        # ãƒã‚¤ã‚ºã‚’é™¤å¤–
        mask = labels != -1
        if mask.sum() < 2:
            return {
                'silhouette_score': 0,
                'calinski_harabasz_score': 0,
                'davies_bouldin_score': 0,
                'n_clusters': 0,
                'n_noise': len(labels)
            }

        filtered_dist = self.combined_dist[mask][:, mask]
        filtered_labels = labels[mask]

        # è©•ä¾¡æŒ‡æ¨™
        metrics = {}

        try:
            # ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°ï¼ˆ-1ã€œ1ã€å¤§ãã„ã»ã©è‰¯ã„ï¼‰
            metrics['silhouette_score'] = silhouette_score(
                filtered_dist, filtered_labels, metric='precomputed'
            )
        except Exception:
            metrics['silhouette_score'] = 0

        # Calinski-Harabaszã¨Davies-Bouldinã¯ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ãŒå¿…è¦
        # åŸ‹ã‚è¾¼ã¿ãŒãªã„å ´åˆã¯MDSã§è·é›¢è¡Œåˆ—ã‹ã‚‰åº§æ¨™ã‚’å¾©å…ƒ
        feature_matrix = None
        if self.data.has_embeddings():
            feature_matrix = self.data.embeddings[mask]
        else:
            try:
                from sklearn.manifold import MDS
                # è·é›¢è¡Œåˆ—ã‹ã‚‰2æ¬¡å…ƒåº§æ¨™ã‚’å¾©å…ƒ
                mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)
                feature_matrix = mds.fit_transform(filtered_dist)
            except Exception:
                feature_matrix = None

        try:
            # Calinski-HarabaszæŒ‡æ•°ï¼ˆå¤§ãã„ã»ã©è‰¯ã„ï¼‰
            if feature_matrix is not None:
                metrics['calinski_harabasz_score'] = calinski_harabasz_score(
                    feature_matrix, filtered_labels
                )
            else:
                metrics['calinski_harabasz_score'] = 0
        except Exception:
            metrics['calinski_harabasz_score'] = 0

        try:
            # Davies-BouldinæŒ‡æ•°ï¼ˆå°ã•ã„ã»ã©è‰¯ã„ï¼‰
            if feature_matrix is not None:
                metrics['davies_bouldin_score'] = davies_bouldin_score(
                    feature_matrix, filtered_labels
                )
            else:
                metrics['davies_bouldin_score'] = 0
        except Exception:
            metrics['davies_bouldin_score'] = 0

        # ã‚¯ãƒ©ã‚¹ã‚¿çµ±è¨ˆ
        metrics['n_clusters'] = len(set(filtered_labels))
        metrics['n_noise'] = list(labels).count(-1)

        return metrics


class ClusterVisualizer:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®å¯è¦–åŒ–"""

    def __init__(self, data: MessageData, labels: np.ndarray):
        self.data = data
        self.labels = labels

    def plot_cluster_distribution(self, output_path: Path):
        """ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚ºåˆ†å¸ƒã‚’å¯è¦–åŒ–"""
        unique_labels = set(self.labels)
        cluster_sizes = [list(self.labels).count(label) for label in unique_labels if label != -1]

        fig, ax = plt.subplots(figsize=(10, 6))
        ax.bar(range(len(cluster_sizes)), sorted(cluster_sizes, reverse=True))
        ax.set_xlabel('ã‚¯ãƒ©ã‚¹ã‚¿IDï¼ˆã‚µã‚¤ã‚ºé †ï¼‰')
        ax.set_ylabel('ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°')
        ax.set_title(f'ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚ºåˆ†å¸ƒï¼ˆåˆè¨ˆ{len(unique_labels) - (1 if -1 in unique_labels else 0)}ã‚¯ãƒ©ã‚¹ã‚¿ï¼‰')
        ax.grid(alpha=0.3)

        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()

    def plot_tsne_projection(self, output_path: Path):
        """t-SNEã«ã‚ˆã‚‹2æ¬¡å…ƒæŠ•å½±ã¨ã‚¯ãƒ©ã‚¹ã‚¿å¯è¦–åŒ–"""
        if not self.data.has_embeddings():
            print("  ! åŸ‹ã‚è¾¼ã¿ãŒç„¡ã„ãŸã‚t-SNEå¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return

        # t-SNEå®Ÿè¡Œ
        tsne = TSNE(n_components=2, random_state=42, perplexity=30)
        embeddings_2d = tsne.fit_transform(self.data.embeddings)

        # å¯è¦–åŒ–
        fig, ax = plt.subplots(figsize=(12, 10))

        unique_labels = set(self.labels)
        colors = plt.cm.tab20(np.linspace(0, 1, len(unique_labels)))

        for label, color in zip(unique_labels, colors):
            if label == -1:
                # ãƒã‚¤ã‚ºã¯é»’ã§ãƒ—ãƒ­ãƒƒãƒˆ
                mask = self.labels == label
                ax.scatter(
                    embeddings_2d[mask, 0], embeddings_2d[mask, 1],
                    c='black', marker='x', s=50, alpha=0.5, label='Noise'
                )
            else:
                mask = self.labels == label
                ax.scatter(
                    embeddings_2d[mask, 0], embeddings_2d[mask, 1],
                    c=[color], s=100, alpha=0.6, label=f'Cluster {label}'
                )

        ax.set_xlabel('t-SNE Component 1')
        ax.set_ylabel('t-SNE Component 2')
        ax.set_title('ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã‚¿ã®2æ¬¡å…ƒæŠ•å½±ï¼ˆt-SNEï¼‰')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)

        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()

    def plot_temporal_clusters(self, output_path: Path):
        """æ™‚ç³»åˆ—ã§ã®ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’å¯è¦–åŒ–"""
        df_with_labels = self.data.df.copy()
        df_with_labels['cluster'] = self.labels

        # ãƒã‚¤ã‚ºã‚’é™¤å¤–
        df_plot = df_with_labels[df_with_labels['cluster'] != -1].copy()

        if len(df_plot) == 0:
            print("  ! æœ‰åŠ¹ãªã‚¯ãƒ©ã‚¹ã‚¿ãŒç„¡ã„ãŸã‚æ™‚ç³»åˆ—å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return

        # æ—¥ä»˜ã”ã¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚«ã‚¦ãƒ³ãƒˆ
        df_plot['date'] = df_plot['start_time'].dt.date
        cluster_counts = df_plot.groupby(['date', 'cluster']).size().unstack(fill_value=0)

        # ãƒ—ãƒ­ãƒƒãƒˆ
        fig, ax = plt.subplots(figsize=(14, 6))
        cluster_counts.plot(kind='area', stacked=True, ax=ax, alpha=0.7)
        ax.set_xlabel('æ—¥ä»˜')
        ax.set_ylabel('ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°')
        ax.set_title('æ™‚ç³»åˆ—ã§ã®ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒ')
        ax.legend(title='ã‚¯ãƒ©ã‚¹ã‚¿', bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(alpha=0.3)

        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()


def run_clustering_with_config(
    csv_path: str,
    embedding_path: Optional[str],
    config: ClusteringConfig,
    generate_embeddings: bool = True
) -> Tuple[MessageData, np.ndarray, Dict]:
    """
    è¨­å®šã«åŸºã¥ã„ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ

    Args:
        csv_path: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸CSVã®ãƒ‘ã‚¹
        embedding_path: åŸ‹ã‚è¾¼ã¿JSONã®ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        config: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š
        generate_embeddings: åŸ‹ã‚è¾¼ã¿ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã‹

    Returns:
        (ãƒ‡ãƒ¼ã‚¿, ãƒ©ãƒ™ãƒ«, è©•ä¾¡æŒ‡æ¨™)
    """
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆåŸ‹ã‚è¾¼ã¿è‡ªå‹•ç”Ÿæˆå«ã‚€ï¼‰
    data = MessageData(csv_path, embedding_path, generate_embeddings=generate_embeddings)

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
    analyzer = ClusterAnalyzer(data, config)
    labels = analyzer.cluster()

    # è©•ä¾¡
    metrics = analyzer.evaluate_clustering(labels)

    return data, labels, metrics


def tune_parameters(
    csv_path: str,
    embedding_path: Optional[str],
    param_grid: Dict,
    generate_embeddings: bool = True
) -> Dict:
    """
    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

    Args:
        csv_path: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸CSVã®ãƒ‘ã‚¹
        embedding_path: åŸ‹ã‚è¾¼ã¿JSONã®ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        param_grid: æ¢ç´¢ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²
        generate_embeddings: åŸ‹ã‚è¾¼ã¿ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã‹

    Returns:
        æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨è©•ä¾¡çµæœ
    """
    results = []

    print("\n" + "=" * 60)
    print("ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
    print("=" * 60)

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆ
    from itertools import product

    keys = param_grid.keys()
    values = param_grid.values()

    for i, combination in enumerate(product(*values)):
        params = dict(zip(keys, combination))

        print(f"\n[{i+1}] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {params}")

        # è¨­å®šä½œæˆ
        config = ClusteringConfig(**params)

        try:
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
            data, labels, metrics = run_clustering_with_config(
                csv_path, embedding_path, config, generate_embeddings=generate_embeddings
            )

            result = {
                'params': params,
                'metrics': metrics,
                'labels': labels
            }
            results.append(result)

            print(f"  çµæœ: {metrics}")

        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸å®šï¼ˆã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°ã‚’åŸºæº–ï¼‰
    best_result = max(results, key=lambda x: x['metrics']['silhouette_score'])

    print("\n" + "=" * 60)
    print("æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    print("=" * 60)
    print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_result['params']}")
    print(f"è©•ä¾¡æŒ‡æ¨™: {best_result['metrics']}")

    return best_result


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import argparse

    parser = argparse.ArgumentParser(description='ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--embedding-weight', type=float, default=0.5, help='åŸ‹ã‚è¾¼ã¿é‡ã¿ (default: 0.5)')
    parser.add_argument('--time-weight', type=float, default=0.2, help='æ™‚é–“é‡ã¿ (default: 0.2)')
    parser.add_argument('--hierarchy-weight', type=float, default=0.3, help='éšå±¤é‡ã¿ (default: 0.3)')
    parser.add_argument('--time-bandwidth-hours', type=float, default=168.0, help='æ™‚é–“ã‚«ãƒ¼ãƒãƒ«å¸¯åŸŸå¹…ï¼ˆæ™‚é–“ï¼‰ (default: 168.0)')
    parser.add_argument('--method', type=str, default='hdbscan', choices=['hdbscan', 'hierarchical', 'kmeans_constrained'], help='ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³• (default: hdbscan)')
    parser.add_argument('--min-cluster-size', type=int, default=5, help='HDBSCANã®æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º (default: 5)')
    parser.add_argument('--min-samples', type=int, default=3, help='HDBSCANã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•° (default: 3)')
    parser.add_argument('--n-clusters', type=int, default=None, help='éšå±¤çš„/k-meansã®ã‚¯ãƒ©ã‚¹ã‚¿æ•° (default: sqrt(n))')
    parser.add_argument('--linkage', type=str, default='average', choices=['average', 'complete', 'single', 'ward'], help='éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®çµåˆæ³• (default: average)')
    parser.add_argument('--size-min', type=int, default=10, help='k-means-constrainedã®æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º (default: 10)')
    parser.add_argument('--size-max', type=int, default=50, help='k-means-constrainedã®æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º (default: 50)')
    parser.add_argument('--n-init', type=int, default=10, help='k-meansã®åˆæœŸåŒ–å›æ•° (default: 10)')
    parser.add_argument('--max-iter', type=int, default=300, help='k-meansã®æœ€å¤§åå¾©å›æ•° (default: 300)')
    args = parser.parse_args()

    print("=" * 60)
    print("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)

    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«
    csv_path = "/Users/mikke/git_dir/chat-line/output/db-exports/2025-11-10T23-54-08/messages_with_hierarchy.csv"
    embedding_path = None  # Noneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆ

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š
    config = ClusteringConfig(
        embedding_weight=args.embedding_weight,
        time_weight=args.time_weight,
        hierarchy_weight=args.hierarchy_weight,
        time_bandwidth_hours=args.time_bandwidth_hours,
        method=args.method,
        min_cluster_size=args.min_cluster_size,
        min_samples=args.min_samples,
        n_clusters=args.n_clusters,
        linkage=args.linkage,
        size_min=args.size_min,
        size_max=args.size_max,
        n_init=args.n_init,
        max_iter=args.max_iter
    )

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
    data, labels, metrics = run_clustering_with_config(
        csv_path, embedding_path, config, generate_embeddings=True
    )

    # å¯è¦–åŒ–
    print("\nå¯è¦–åŒ–ã‚’ç”Ÿæˆä¸­...")
    visualizer = ClusterVisualizer(data, labels)
    visualizer.plot_cluster_distribution(OUTPUT_DIR / "cluster_distribution.png")
    visualizer.plot_tsne_projection(OUTPUT_DIR / "tsne_projection.png")
    visualizer.plot_temporal_clusters(OUTPUT_DIR / "temporal_clusters.png")
    print("  âœ“ å¯è¦–åŒ–å®Œäº†")

    # çµæœã‚’DataFrameã«ä¿å­˜
    df_result = data.df.copy()
    df_result['cluster'] = labels
    output_csv = OUTPUT_DIR / "clustered_messages.csv"
    df_result.to_csv(output_csv, index=False, encoding='utf-8')
    print(f"\nâœ“ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ä¿å­˜: {output_csv}")

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨è¨­å®šã‚’ä¿å­˜
    result_metadata = {
        'metrics': metrics,
        'config': {
            'embedding_weight': config.embedding_weight,
            'time_weight': config.time_weight,
            'hierarchy_weight': config.hierarchy_weight,
            'time_bandwidth_hours': config.time_bandwidth_hours,
            'method': config.method,
            'min_cluster_size': config.min_cluster_size,
            'min_samples': config.min_samples
        }
    }
    metadata_path = OUTPUT_DIR / "clustering_metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(result_metadata, f, ensure_ascii=False, indent=2)
    print(f"âœ“ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨è¨­å®šã‚’ä¿å­˜: {metadata_path}")

    print("\n" + "=" * 60)
    print("âœ… ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†ï¼")
    print("=" * 60)
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
