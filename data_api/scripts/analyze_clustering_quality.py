#!/usr/bin/env python3
"""
ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ªã®è©³ç´°åˆ†æ

1. åŸ‹ã‚è¾¼ã¿è·é›¢ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
2. éšå±¤ã¨ã‚¯ãƒ©ã‚¹ã‚¿ã®é–¢ä¿‚åˆ†æ
3. ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®åŸ‹ã‚è¾¼ã¿é¡ä¼¼åº¦åˆ†æ
"""

import json
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'Hiragino Sans'
plt.rcParams['axes.unicode_minus'] = False

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
OUTPUT_DIR = Path("output/clustering_analysis")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def load_data():
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ
    df = pd.read_csv("output/message_clustering/clustered_messages.csv")

    # åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿
    with open("output/message_clustering/messages_embedded.json", 'r', encoding='utf-8') as f:
        embedding_data = json.load(f)

    # message_idã¨embeddingã®å¯¾å¿œ
    embedding_dict = {item['id']: item['embedding'] for item in embedding_data}

    # DataFrameã®é †åºã«åˆã‚ã›ã¦åŸ‹ã‚è¾¼ã¿ã‚’é…ç½®
    embeddings_list = []
    for msg_id in df['message_id']:
        if msg_id in embedding_dict:
            embeddings_list.append(embedding_dict[msg_id])
        else:
            embeddings_list.append([0.0] * 1024)

    embeddings = np.array(embeddings_list)

    return df, embeddings


def analyze_embedding_distances(embeddings):
    """åŸ‹ã‚è¾¼ã¿è·é›¢ã®åˆ†æ"""
    print("\n" + "="*60)
    print("åŸ‹ã‚è¾¼ã¿è·é›¢ã®åˆ†æ")
    print("="*60)

    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
    similarity = cosine_similarity(embeddings)

    # è·é›¢ã«å¤‰æ›ï¼ˆ0ã€œ2ã®ç¯„å›²ï¼‰
    distance = 1 - similarity

    # ä¸Šä¸‰è§’ã®ã¿å–å¾—ï¼ˆå¯¾è§’ç·šã‚’é™¤ãï¼‰
    triu_indices = np.triu_indices_from(distance, k=1)
    distances_flat = distance[triu_indices]

    # çµ±è¨ˆæƒ…å ±
    print("\nåŸ‹ã‚è¾¼ã¿è·é›¢ã®çµ±è¨ˆ:")
    print(f"  å¹³å‡: {distances_flat.mean():.4f}")
    print(f"  ä¸­å¤®å€¤: {np.median(distances_flat):.4f}")
    print(f"  æœ€å°: {distances_flat.min():.4f}")
    print(f"  æœ€å¤§: {distances_flat.max():.4f}")
    print(f"  æ¨™æº–åå·®: {distances_flat.std():.4f}")

    # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
    percentiles = [10, 25, 50, 75, 90, 95, 99]
    print("\nãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«:")
    for p in percentiles:
        val = np.percentile(distances_flat, p)
        print(f"  {p}%: {val:.4f}")

    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ä½œæˆ
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # å…¨ä½“ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    ax = axes[0, 0]
    ax.hist(distances_flat, bins=100, alpha=0.7, color='blue', edgecolor='black')
    ax.axvline(distances_flat.mean(), color='red', linestyle='--', linewidth=2, label=f'å¹³å‡: {distances_flat.mean():.4f}')
    ax.axvline(np.median(distances_flat), color='green', linestyle='--', linewidth=2, label=f'ä¸­å¤®å€¤: {np.median(distances_flat):.4f}')
    ax.set_xlabel('åŸ‹ã‚è¾¼ã¿è·é›¢ï¼ˆã‚³ã‚µã‚¤ãƒ³è·é›¢ï¼‰')
    ax.set_ylabel('ãƒšã‚¢æ•°')
    ax.set_title('åŸ‹ã‚è¾¼ã¿è·é›¢ã®åˆ†å¸ƒï¼ˆå…¨ä½“ï¼‰')
    ax.legend()
    ax.grid(alpha=0.3)

    # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«
    ax = axes[0, 1]
    ax.hist(distances_flat, bins=100, alpha=0.7, color='blue', edgecolor='black')
    ax.set_xlabel('åŸ‹ã‚è¾¼ã¿è·é›¢ï¼ˆã‚³ã‚µã‚¤ãƒ³è·é›¢ï¼‰')
    ax.set_ylabel('ãƒšã‚¢æ•°ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰')
    ax.set_yscale('log')
    ax.set_title('åŸ‹ã‚è¾¼ã¿è·é›¢ã®åˆ†å¸ƒï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰')
    ax.grid(alpha=0.3)

    # ç´¯ç©åˆ†å¸ƒ
    ax = axes[1, 0]
    sorted_distances = np.sort(distances_flat)
    cumulative = np.arange(1, len(sorted_distances) + 1) / len(sorted_distances) * 100
    ax.plot(sorted_distances, cumulative, linewidth=2)
    ax.set_xlabel('åŸ‹ã‚è¾¼ã¿è·é›¢ï¼ˆã‚³ã‚µã‚¤ãƒ³è·é›¢ï¼‰')
    ax.set_ylabel('ç´¯ç©ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« (%)')
    ax.set_title('åŸ‹ã‚è¾¼ã¿è·é›¢ã®ç´¯ç©åˆ†å¸ƒ')
    ax.grid(alpha=0.3)

    # è¿‘ã„è·é›¢ã®ã¿ï¼ˆè·é›¢ < 0.5ï¼‰
    ax = axes[1, 1]
    close_distances = distances_flat[distances_flat < 0.5]
    if len(close_distances) > 0:
        ax.hist(close_distances, bins=50, alpha=0.7, color='green', edgecolor='black')
        ax.set_xlabel('åŸ‹ã‚è¾¼ã¿è·é›¢ï¼ˆã‚³ã‚µã‚¤ãƒ³è·é›¢ï¼‰')
        ax.set_ylabel('ãƒšã‚¢æ•°')
        ax.set_title(f'è¿‘ã„è·é›¢ã®ã¿ï¼ˆ< 0.5ï¼‰: {len(close_distances)}ãƒšã‚¢')
        ax.grid(alpha=0.3)
    else:
        ax.text(0.5, 0.5, 'è·é›¢ < 0.5 ã®ãƒšã‚¢ãªã—', ha='center', va='center', transform=ax.transAxes)

    plt.tight_layout()
    output_path = OUTPUT_DIR / "embedding_distance_histogram.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"\nâœ“ ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä¿å­˜: {output_path}")

    return distance


def analyze_hierarchy_cluster_relationship(df):
    """éšå±¤ã¨ã‚¯ãƒ©ã‚¹ã‚¿ã®é–¢ä¿‚ã‚’åˆ†æ"""
    print("\n" + "="*60)
    print("éšå±¤ã¨ã‚¯ãƒ©ã‚¹ã‚¿ã®é–¢ä¿‚åˆ†æ")
    print("="*60)

    # éšå±¤ï¼ˆãƒãƒ£ãƒãƒ«ï¼‰ã”ã¨ã®ã‚¯ãƒ©ã‚¹ã‚¿æ•£ã‚‰ã°ã‚Š
    hierarchy_cluster_mapping = {}

    for hierarchy in df['normalized_path'].unique():
        hierarchy_df = df[df['normalized_path'] == hierarchy]
        clusters = hierarchy_df['cluster'].unique()

        hierarchy_cluster_mapping[hierarchy] = {
            'n_messages': len(hierarchy_df),
            'n_clusters': len(clusters),
            'clusters': sorted(clusters.tolist())
        }

    # 3ã¤ä»¥ä¸Šã®ã‚¯ãƒ©ã‚¹ã‚¿ã«æ•£ã‚‰ã°ã£ã¦ã„ã‚‹éšå±¤
    scattered_hierarchies = {k: v for k, v in hierarchy_cluster_mapping.items() if v['n_clusters'] >= 3}

    print(f"\néšå±¤ã®ç·æ•°: {len(hierarchy_cluster_mapping)}")
    print(f"3ã¤ä»¥ä¸Šã®ã‚¯ãƒ©ã‚¹ã‚¿ã«æ•£ã‚‰ã°ã£ã¦ã„ã‚‹éšå±¤: {len(scattered_hierarchies)}")

    # æ•£ã‚‰ã°ã‚Šåº¦åˆã„ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    n_clusters_list = [v['n_clusters'] for v in hierarchy_cluster_mapping.values()]

    print("\néšå±¤ã”ã¨ã®ã‚¯ãƒ©ã‚¹ã‚¿æ•°åˆ†å¸ƒ:")
    for n in range(1, max(n_clusters_list) + 1):
        count = n_clusters_list.count(n)
        print(f"  {n}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿: {count}éšå±¤")

    # è©³ç´°è¡¨ç¤ºï¼ˆ3ã¤ä»¥ä¸Šã«æ•£ã‚‰ã°ã£ã¦ã„ã‚‹å ´åˆï¼‰
    if scattered_hierarchies:
        print("\n3ã¤ä»¥ä¸Šã®ã‚¯ãƒ©ã‚¹ã‚¿ã«æ•£ã‚‰ã°ã£ã¦ã„ã‚‹éšå±¤ã®è©³ç´°:")
        for hierarchy, info in sorted(scattered_hierarchies.items(), key=lambda x: x[1]['n_clusters'], reverse=True):
            print(f"\n  éšå±¤: {hierarchy}")
            print(f"    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {info['n_messages']}")
            print(f"    ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {info['n_clusters']}")
            print(f"    ã‚¯ãƒ©ã‚¹ã‚¿ID: {info['clusters']}")
    else:
        print("\nâš ï¸ 3ã¤ä»¥ä¸Šã®ã‚¯ãƒ©ã‚¹ã‚¿ã«æ•£ã‚‰ã°ã£ã¦ã„ã‚‹éšå±¤ã¯0ä»¶ã§ã™")

    # å¯è¦–åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    ax = axes[0, 0]
    ax.hist(n_clusters_list, bins=range(1, max(n_clusters_list) + 2), alpha=0.7, color='blue', edgecolor='black')
    ax.set_xlabel('1ã¤ã®éšå±¤ãŒæ•£ã‚‰ã°ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿æ•°')
    ax.set_ylabel('éšå±¤æ•°')
    ax.set_title('éšå±¤ã”ã¨ã®ã‚¯ãƒ©ã‚¹ã‚¿æ•£ã‚‰ã°ã‚Šåº¦')
    ax.grid(alpha=0.3)

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•° vs ã‚¯ãƒ©ã‚¹ã‚¿æ•°
    ax = axes[0, 1]
    n_messages_list = [v['n_messages'] for v in hierarchy_cluster_mapping.values()]
    ax.scatter(n_messages_list, n_clusters_list, alpha=0.6)
    ax.set_xlabel('éšå±¤å†…ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°')
    ax.set_ylabel('æ•£ã‚‰ã°ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿æ•°')
    ax.set_title('ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã¨ã‚¯ãƒ©ã‚¹ã‚¿æ•£ã‚‰ã°ã‚Šã®é–¢ä¿‚')
    ax.grid(alpha=0.3)

    # ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®éšå±¤æ•°
    ax = axes[1, 0]
    cluster_hierarchy_counts = df.groupby('cluster')['normalized_path'].nunique().sort_values(ascending=False)
    ax.bar(range(len(cluster_hierarchy_counts)), cluster_hierarchy_counts.values, alpha=0.7, color='green', edgecolor='black')
    ax.set_xlabel('ã‚¯ãƒ©ã‚¹ã‚¿ID')
    ax.set_ylabel('å«ã¾ã‚Œã‚‹éšå±¤æ•°')
    ax.set_title('å„ã‚¯ãƒ©ã‚¹ã‚¿ã«å«ã¾ã‚Œã‚‹éšå±¤ã®æ•°')
    ax.set_xticks(range(len(cluster_hierarchy_counts)))
    ax.set_xticklabels([f"C{i}" for i in cluster_hierarchy_counts.index], rotation=45)
    ax.grid(alpha=0.3)

    # ã‚¯ãƒ­ã‚¹é›†è¨ˆã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆä¸Šä½10éšå±¤ã®ã¿ï¼‰
    ax = axes[1, 1]
    top_hierarchies = df['normalized_path'].value_counts().head(10).index
    df_top = df[df['normalized_path'].isin(top_hierarchies)]
    crosstab = pd.crosstab(df_top['normalized_path'], df_top['cluster'])

    sns.heatmap(crosstab, cmap='YlOrRd', annot=True, fmt='d', ax=ax, cbar_kws={'label': 'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°'})
    ax.set_xlabel('ã‚¯ãƒ©ã‚¹ã‚¿ID')
    ax.set_ylabel('éšå±¤ï¼ˆä¸Šä½10ä»¶ï¼‰')
    ax.set_title('éšå±¤ Ã— ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒï¼ˆä¸Šä½10éšå±¤ï¼‰')

    plt.tight_layout()
    output_path = OUTPUT_DIR / "hierarchy_cluster_relationship.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"\nâœ“ é–¢ä¿‚å›³ã‚’ä¿å­˜: {output_path}")

    return hierarchy_cluster_mapping


def analyze_intra_cluster_similarity(df, embeddings):
    """ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®åŸ‹ã‚è¾¼ã¿é¡ä¼¼åº¦ã‚’åˆ†æ"""
    print("\n" + "="*60)
    print("ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®åŸ‹ã‚è¾¼ã¿é¡ä¼¼åº¦åˆ†æ")
    print("="*60)

    cluster_similarities = {}

    for cluster_id in sorted(df['cluster'].unique()):
        cluster_mask = df['cluster'] == cluster_id
        cluster_embeddings = embeddings[cluster_mask]

        if len(cluster_embeddings) < 2:
            continue

        # ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
        similarity = cosine_similarity(cluster_embeddings)

        # ä¸Šä¸‰è§’ã®ã¿ï¼ˆå¯¾è§’ç·šã‚’é™¤ãï¼‰
        triu_indices = np.triu_indices_from(similarity, k=1)
        similarities_flat = similarity[triu_indices]

        cluster_similarities[cluster_id] = {
            'size': len(cluster_embeddings),
            'mean_similarity': similarities_flat.mean(),
            'std_similarity': similarities_flat.std(),
            'min_similarity': similarities_flat.min(),
            'max_similarity': similarities_flat.max()
        }

    # çµæœè¡¨ç¤º
    print("\nã‚¯ãƒ©ã‚¹ã‚¿å†…ã®å¹³å‡ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦:")
    for cluster_id, stats in sorted(cluster_similarities.items()):
        print(f"  ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}: å¹³å‡={stats['mean_similarity']:.4f}, "
              f"æ¨™æº–åå·®={stats['std_similarity']:.4f}, "
              f"ç¯„å›²=[{stats['min_similarity']:.4f}, {stats['max_similarity']:.4f}], "
              f"ã‚µã‚¤ã‚º={stats['size']}")

    # å¯è¦–åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®å¹³å‡é¡ä¼¼åº¦
    ax = axes[0]
    cluster_ids = list(cluster_similarities.keys())
    mean_sims = [cluster_similarities[c]['mean_similarity'] for c in cluster_ids]
    colors = ['red' if s < 0.5 else 'orange' if s < 0.7 else 'green' for s in mean_sims]

    ax.bar(range(len(cluster_ids)), mean_sims, alpha=0.7, color=colors, edgecolor='black')
    ax.axhline(0.5, color='red', linestyle='--', linewidth=1, label='ä½ã„é¡ä¼¼åº¦ (< 0.5)')
    ax.axhline(0.7, color='orange', linestyle='--', linewidth=1, label='ä¸­ç¨‹åº¦ (0.5-0.7)')
    ax.set_xlabel('ã‚¯ãƒ©ã‚¹ã‚¿ID')
    ax.set_ylabel('å¹³å‡ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦')
    ax.set_title('ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®åŸ‹ã‚è¾¼ã¿é¡ä¼¼åº¦ï¼ˆå¹³å‡ï¼‰')
    ax.set_xticks(range(len(cluster_ids)))
    ax.set_xticklabels([f"C{c}" for c in cluster_ids], rotation=45)
    ax.legend()
    ax.grid(alpha=0.3)

    # é¡ä¼¼åº¦ã®åˆ†å¸ƒï¼ˆå…¨ã‚¯ãƒ©ã‚¹ã‚¿ï¼‰
    ax = axes[1]
    all_mean_sims = [s['mean_similarity'] for s in cluster_similarities.values()]
    ax.hist(all_mean_sims, bins=20, alpha=0.7, color='blue', edgecolor='black')
    ax.axvline(np.mean(all_mean_sims), color='red', linestyle='--', linewidth=2,
               label=f'å…¨ä½“å¹³å‡: {np.mean(all_mean_sims):.4f}')
    ax.set_xlabel('ã‚¯ãƒ©ã‚¹ã‚¿å†…å¹³å‡ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦')
    ax.set_ylabel('ã‚¯ãƒ©ã‚¹ã‚¿æ•°')
    ax.set_title('ã‚¯ãƒ©ã‚¹ã‚¿å†…é¡ä¼¼åº¦ã®åˆ†å¸ƒ')
    ax.legend()
    ax.grid(alpha=0.3)

    plt.tight_layout()
    output_path = OUTPUT_DIR / "intra_cluster_similarity.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"\nâœ“ é¡ä¼¼åº¦åˆ†æã‚’ä¿å­˜: {output_path}")

    return cluster_similarities


def generate_summary_report(distance_stats, hierarchy_mapping, cluster_similarities):
    """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    report_path = OUTPUT_DIR / "clustering_quality_report.txt"

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ªåˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n")
        f.write("="*60 + "\n\n")

        # åŸ‹ã‚è¾¼ã¿è·é›¢
        f.write("ã€åŸ‹ã‚è¾¼ã¿è·é›¢ã®çµ±è¨ˆã€‘\n")
        f.write("  å…¨ãƒšã‚¢ã®è·é›¢åˆ†å¸ƒã‚’åˆ†æ\n")
        f.write("  è©³ç´°ã¯ã‚°ãƒ©ãƒ•ã‚’å‚ç…§: embedding_distance_histogram.png\n\n")

        # éšå±¤ã¨ã‚¯ãƒ©ã‚¹ã‚¿ã®é–¢ä¿‚
        f.write("ã€éšå±¤ã¨ã‚¯ãƒ©ã‚¹ã‚¿ã®é–¢ä¿‚ã€‘\n")
        f.write(f"  ç·éšå±¤æ•°: {len(hierarchy_mapping)}\n")

        scattered_count = sum(1 for v in hierarchy_mapping.values() if v['n_clusters'] >= 3)
        f.write(f"  3ã¤ä»¥ä¸Šã®ã‚¯ãƒ©ã‚¹ã‚¿ã«æ•£ã‚‰ã°ã‚‹éšå±¤: {scattered_count}\n")

        if scattered_count == 0:
            f.write("\n  âš ï¸ è­¦å‘Š: éšå±¤ãŒã‚¯ãƒ©ã‚¹ã‚¿ã«æ•£ã‚‰ã°ã£ã¦ã„ã¾ã›ã‚“\n")
            f.write("  â†’ åŸ‹ã‚è¾¼ã¿ã‚ˆã‚Šã‚‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆéšå±¤ãƒ»æ™‚é–“ï¼‰ã®å½±éŸ¿ãŒå¼·ã™ãã‚‹å¯èƒ½æ€§\n")
            f.write("  â†’ é‡ã¿ã¥ã‘ã®èª¿æ•´ã‚’æ¨å¥¨ï¼ˆåŸ‹ã‚è¾¼ã¿ã®é‡ã¿ã‚’å¢—ã‚„ã™ï¼‰\n\n")

        # ã‚¯ãƒ©ã‚¹ã‚¿å†…é¡ä¼¼åº¦
        f.write("ã€ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®åŸ‹ã‚è¾¼ã¿é¡ä¼¼åº¦ã€‘\n")
        mean_sims = [s['mean_similarity'] for s in cluster_similarities.values()]
        f.write(f"  å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã®å¹³å‡: {np.mean(mean_sims):.4f}\n")
        f.write(f"  æœ€å°: {min(mean_sims):.4f}\n")
        f.write(f"  æœ€å¤§: {max(mean_sims):.4f}\n\n")

        low_sim_clusters = [c for c, s in cluster_similarities.items() if s['mean_similarity'] < 0.5]
        if low_sim_clusters:
            f.write(f"  âš ï¸ ä½ã„é¡ä¼¼åº¦(<0.5)ã®ã‚¯ãƒ©ã‚¹ã‚¿: {low_sim_clusters}\n")
            f.write("  â†’ ã“ã‚Œã‚‰ã®ã‚¯ãƒ©ã‚¹ã‚¿ã¯æ„å‘³çš„ã«å¤šæ§˜ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚€\n\n")

        f.write("ã€æ¨å¥¨äº‹é …ã€‘\n")
        if scattered_count == 0:
            f.write("  1. åŸ‹ã‚è¾¼ã¿ã®é‡ã¿ã‚’å¢—ã‚„ã™ï¼ˆä¾‹: 0.7ä»¥ä¸Šï¼‰\n")
            f.write("  2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’æ¸›ã‚‰ã™ï¼ˆæ™‚é–“ãƒ»éšå±¤ã®åˆè¨ˆã‚’0.3ä»¥ä¸‹ï¼‰\n")
        else:
            f.write("  1. ç¾åœ¨ã®é‡ã¿ã¥ã‘ã¯é©åˆ‡ã§ã™\n")
            f.write("  2. ã‚¯ãƒ©ã‚¹ã‚¿å†…é¡ä¼¼åº¦ãŒä½ã„å ´åˆã¯ã€min_cluster_sizeã‚’èª¿æ•´ã—ã¦ãã ã•ã„\n")

    print(f"\nâœ“ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("="*60)
    print("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ªã®è©³ç´°åˆ†æ")
    print("="*60)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    df, embeddings = load_data()
    print(f"âœ“ {len(df)}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨åŸ‹ã‚è¾¼ã¿ï¼ˆ{embeddings.shape[1]}æ¬¡å…ƒï¼‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # åŸ‹ã‚è¾¼ã¿è·é›¢ã®åˆ†æ
    distance_matrix = analyze_embedding_distances(embeddings)

    # éšå±¤ã¨ã‚¯ãƒ©ã‚¹ã‚¿ã®é–¢ä¿‚åˆ†æ
    hierarchy_mapping = analyze_hierarchy_cluster_relationship(df)

    # ã‚¯ãƒ©ã‚¹ã‚¿å†…é¡ä¼¼åº¦ã®åˆ†æ
    cluster_similarities = analyze_intra_cluster_similarity(df, embeddings)

    # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_summary_report(distance_matrix, hierarchy_mapping, cluster_similarities)

    print("\n" + "="*60)
    print("âœ… åˆ†æå®Œäº†ï¼")
    print("="*60)
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {OUTPUT_DIR}")
    print("\nç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
    print("  - embedding_distance_histogram.png (åŸ‹ã‚è¾¼ã¿è·é›¢ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ )")
    print("  - hierarchy_cluster_relationship.png (éšå±¤ã¨ã‚¯ãƒ©ã‚¹ã‚¿ã®é–¢ä¿‚)")
    print("  - intra_cluster_similarity.png (ã‚¯ãƒ©ã‚¹ã‚¿å†…é¡ä¼¼åº¦)")
    print("  - clustering_quality_report.txt (ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ)")


if __name__ == "__main__":
    main()
