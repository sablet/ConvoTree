#!/usr/bin/env python3
"""
ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®æ„å›³æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ

å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æ„å›³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ

ä½¿ç”¨ä¾‹:
  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ç”Ÿæˆ
  python scripts/generate_intent_extraction_prompts.py

  # Gemini APIã§æ„å›³æŠ½å‡ºã‚’å®Ÿè¡Œã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨HTMLã‚’ç”Ÿæˆ
  python scripts/generate_intent_extraction_prompts.py --gemini
"""

import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import argparse
import os
import sys
from dotenv import load_dotenv
from tqdm import tqdm

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))
from lib import gemini_client


OUTPUT_DIR = Path("output/intent_extraction")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

TEMPLATE_DIR = Path("templates")
TEMPLATE_FILE = TEMPLATE_DIR / "intent_extraction_prompt.md"
GROUPING_TEMPLATE_FILE = TEMPLATE_DIR / "intent_grouping_prompt.md"
REASSIGNMENT_TEMPLATE_FILE = TEMPLATE_DIR / "intent_reassignment_prompt.md"
COMMON_INTENT_OBJECT_FILE = TEMPLATE_DIR / "common" / "intent_object_common.md"

PROCESSED_DIR = OUTPUT_DIR / "processed"
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

AGGREGATED_DIR = OUTPUT_DIR / "aggregated"
AGGREGATED_DIR.mkdir(parents=True, exist_ok=True)

CROSS_CLUSTER_DIR = OUTPUT_DIR / "cross_cluster"
CROSS_CLUSTER_DIR.mkdir(parents=True, exist_ok=True)


def load_common_intent_object_definition() -> str:
    """å…±é€šintent objectå®šç¾©ã‚’èª­ã¿è¾¼ã¿"""
    if not COMMON_INTENT_OBJECT_FILE.exists():
        raise FileNotFoundError(
            f"å…±é€šå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {COMMON_INTENT_OBJECT_FILE}"
        )

    with open(COMMON_INTENT_OBJECT_FILE, "r", encoding="utf-8") as f:
        return f.read()


def load_template() -> str:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿"""
    if not TEMPLATE_FILE.exists():
        raise FileNotFoundError(
            f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {TEMPLATE_FILE}"
        )

    with open(TEMPLATE_FILE, "r", encoding="utf-8") as f:
        template = f.read()

    # å…±é€šå®šç¾©ã‚’å·®ã—è¾¼ã¿
    common_definition = load_common_intent_object_definition()
    template = template.replace("{COMMON_INTENT_OBJECT}", common_definition)

    return template


def load_grouping_template() -> str:
    """æ„å›³ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿"""
    if not GROUPING_TEMPLATE_FILE.exists():
        raise FileNotFoundError(
            f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {GROUPING_TEMPLATE_FILE}"
        )

    with open(GROUPING_TEMPLATE_FILE, "r", encoding="utf-8") as f:
        template = f.read()

    # å…±é€šå®šç¾©ã‚’å·®ã—è¾¼ã¿
    common_definition = load_common_intent_object_definition()
    template = template.replace("{COMMON_INTENT_OBJECT}", common_definition)

    return template


def load_reassignment_template() -> str:
    """æ„å›³å†å‰²ã‚ŠæŒ¯ã‚Šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿"""
    if not REASSIGNMENT_TEMPLATE_FILE.exists():
        raise FileNotFoundError(
            f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {REASSIGNMENT_TEMPLATE_FILE}"
        )

    with open(REASSIGNMENT_TEMPLATE_FILE, "r", encoding="utf-8") as f:
        template = f.read()

    return template


def load_clustered_messages() -> pd.DataFrame:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿"""
    csv_path = Path("output/message_clustering/clustered_messages.csv")
    if not csv_path.exists():
        raise FileNotFoundError(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")

    df = pd.read_csv(csv_path)
    df["start_time"] = pd.to_datetime(df["start_time"])
    return df


def build_message_metadata(df: pd.DataFrame) -> Dict[str, Dict]:
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸IDã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰

    Args:
        df: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®DataFrame

    Returns:
        msg_id -> {full_path, min_start_timestamp} ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    """
    metadata = {}
    for row in df.itertuples():
        msg_id = row.message_id
        if msg_id not in metadata:
            metadata[msg_id] = {
                "full_path": row.full_path,
                "min_start_timestamp": row.start_time.isoformat(),
            }
        else:
            # åŒã˜msg_idã§è¤‡æ•°è¡Œã‚ã‚‹å ´åˆã€æœ€å°ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä¿æŒ
            current_ts = pd.to_datetime(metadata[msg_id]["min_start_timestamp"])
            if row.start_time < current_ts:
                metadata[msg_id]["min_start_timestamp"] = row.start_time.isoformat()
    return metadata


def generate_cluster_prompt(
    cluster_id: int, cluster_df: pd.DataFrame, template: str
) -> Dict:
    """
    1ã¤ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«å¯¾ã™ã‚‹æ„å›³æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ

    Args:
        cluster_id: ã‚¯ãƒ©ã‚¹ã‚¿ID
        cluster_df: ã‚¯ãƒ©ã‚¹ã‚¿ã«å±ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®DataFrame
        template: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

    Returns:
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±ã®è¾æ›¸
    """
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆ
    cluster_df = cluster_df.sort_values("start_time")

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
    messages = []
    message_parts = []

    for i, row in enumerate(cluster_df.itertuples(), 1):
        msg = {
            "message_id": row.message_id,
            "channel": row.full_path,
            "time": row.start_time.strftime("%Y-%m-%d %H:%M"),
            "content": row.combined_content,
        }
        messages.append(msg)

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸éƒ¨åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        message_parts.extend(
            [
                f"### ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {i}",
                f"- ID: `{msg['message_id']}`",
                f"- ãƒãƒ£ãƒãƒ«: {msg['channel']}",
                f"- æ—¥æ™‚: {msg['time']}",
                "- å†…å®¹:",
                "```",
                msg["content"],
                "```",
                "",
            ]
        )

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«å€¤ã‚’åŸ‹ã‚è¾¼ã¿
    prompt_text = template.format(
        cluster_id=cluster_id,
        message_count=len(messages),
        period_start=cluster_df["start_time"].min().strftime("%Y-%m-%d"),
        period_end=cluster_df["start_time"].max().strftime("%Y-%m-%d"),
        messages="\n".join(message_parts),
    )

    return {
        "cluster_id": cluster_id,
        "message_count": len(messages),
        "prompt": prompt_text,
        "messages": messages,
    }


def preprocess_extract_json_from_response(text: str) -> Optional[List[Dict]]:
    """
    ã€å‰å‡¦ç†ã€‘ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰JSONã‚’æŠ½å‡ºã—ã¦ãƒ‘ãƒ¼ã‚¹

    Args:
        text: Gemini APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸJSONï¼ˆãƒªã‚¹ãƒˆï¼‰ã¾ãŸã¯None
    """
    text = text.strip()

    # ```json ... ``` ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
    if "```json" in text:
        start = text.find("```json") + 7
        end = text.find("```", start)
        json_text = text[start:end].strip()
    elif "```" in text:
        start = text.find("```") + 3
        end = text.find("```", start)
        json_text = text[start:end].strip()
    else:
        json_text = text

    # JSONãƒ‘ãƒ¼ã‚¹
    try:
        result = json.loads(json_text)
        if not isinstance(result, list):
            return None
        return result
    except json.JSONDecodeError:
        return None


def postprocess_enrich_and_save_intents(
    raw_response_text: str, cluster_id: int, message_metadata: Dict[str, Dict]
) -> Optional[List[Dict]]:
    """
    ã€å¾Œå‡¦ç†ã€‘ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡ºã—ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è£œå®Œã—ã¦ä¿å­˜

    Args:
        raw_response_text: Gemini APIã®ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹
        cluster_id: ã‚¯ãƒ©ã‚¹ã‚¿ID
        message_metadata: msg_id -> {full_path, min_start_timestamp} ã®ãƒãƒƒãƒ”ãƒ³ã‚°

    Returns:
        å‡¦ç†å¾Œã®æ„å›³ãƒªã‚¹ãƒˆã€ã¾ãŸã¯None
    """
    # JSONã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆå‰å‡¦ç†ï¼‰
    intents = preprocess_extract_json_from_response(raw_response_text)
    if intents is None:
        return None

    # å„æ„å›³ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    for intent in intents:
        # cluster_idã‚’è¿½åŠ ï¼ˆint64 -> int å¤‰æ›ï¼‰
        intent["cluster_id"] = int(cluster_id)

        source_ids = intent.get("source_message_ids", [])
        if not source_ids:
            continue

        # source_message_idsã«å¯¾å¿œã™ã‚‹full_pathã¨min_start_timestampã‚’é›†ç´„
        full_paths = []
        timestamps = []

        for msg_id in source_ids:
            metadata = message_metadata.get(msg_id, {})
            full_path = metadata.get("full_path")
            timestamp = metadata.get("min_start_timestamp")

            if full_path:
                full_paths.append(full_path)
            if timestamp:
                timestamps.append(timestamp)

        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªfull_pathã®ãƒªã‚¹ãƒˆ
        intent["source_full_paths"] = list(set(full_paths)) if full_paths else []

        # æœ€å°ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        intent["min_start_timestamp"] = min(timestamps) if timestamps else None

    # å‡¦ç†å¾Œã®JSONã‚’ä¿å­˜
    output_file = PROCESSED_DIR / f"cluster_{cluster_id:02d}_processed.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(intents, f, ensure_ascii=False, indent=2)

    return intents


def reassign_uncovered_items(
    existing_groups: List[Dict],
    uncovered_indices: set,
    original_items: List[Dict],
    reassignment_template: str,
    cluster_id: Optional[int] = None,
    save_raw: bool = False,
    level_name: str = "intent",
    max_retries: int = 3,
) -> List[Dict]:
    """
    ã€å†å‰²ã‚ŠæŒ¯ã‚Šã€‘æœªã‚«ãƒãƒ¼ã®é …ç›®ã‚’æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ ã¾ãŸã¯æ–°è¦ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ

    Args:
        existing_groups: æ—¢å­˜ã®ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆmeta_intents, super_intents, ultra_intentsãªã©ï¼‰
        uncovered_indices: æœªã‚«ãƒãƒ¼ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚»ãƒƒãƒˆ
        original_items: å…ƒã®é …ç›®ãƒªã‚¹ãƒˆï¼ˆintents, meta_intents, super_intentsãªã©ï¼‰
        reassignment_template: å†å‰²ã‚ŠæŒ¯ã‚Šç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        cluster_id: ã‚¯ãƒ©ã‚¹ã‚¿IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ã®å ´åˆã¯Noneï¼‰
        save_raw: ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¿å­˜ãƒ•ãƒ©ã‚°
        level_name: ãƒ¬ãƒ™ãƒ«åï¼ˆ"meta", "super", "ultra"ãªã©ã€ãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        max_retries: æœ€å¤§è©¦è¡Œå›æ•°

    Returns:
        æ›´æ–°ã•ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒªã‚¹ãƒˆ
    """
    if not uncovered_indices:
        return existing_groups

    retry_count = 0
    current_uncovered = uncovered_indices.copy()

    while current_uncovered and retry_count < max_retries:
        retry_count += 1
        print(
            f"\nğŸ”„ å†å‰²ã‚ŠæŒ¯ã‚Šè©¦è¡Œ {retry_count}/{max_retries} - {len(current_uncovered)}ä»¶ã®æœªã‚«ãƒãƒ¼é …ç›®"
        )

        # æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã®è¦ç´„ã‚’ä½œæˆ
        existing_group_texts = []
        for i, group in enumerate(existing_groups):
            group_name_key = (
                "meta_intent"
                if "meta_intent" in group
                else "super_intent"
                if "super_intent" in group
                else "ultra_intent"
            )
            group_name = group.get(group_name_key, f"ã‚°ãƒ«ãƒ¼ãƒ— {i}")

            # ã‚°ãƒ«ãƒ¼ãƒ—ã®æ—¢å­˜ãƒ¡ãƒ³ãƒãƒ¼æ•°ã‚’è¡¨ç¤º
            member_key = (
                "covered_intent_ids"
                if "covered_intent_ids" in group
                else "covered_meta_intent_indices"
                if "covered_meta_intent_indices" in group
                else "covered_super_intent_indices"
                if "covered_super_intent_indices" in group
                else "member_indices"
            )
            existing_members = group.get(member_key, [])
            existing_group_texts.append(
                f"{i}. {group_name} (æ—¢å­˜ãƒ¡ãƒ³ãƒãƒ¼: {len(existing_members)}ä»¶)"
            )

        existing_groups_summary = "\n".join(existing_group_texts)

        # æœªã‚«ãƒãƒ¼é …ç›®ã®è©³ç´°ã‚’ä½œæˆ
        uncovered_item_texts = []
        uncovered_list = sorted(current_uncovered)

        for i, original_idx in enumerate(uncovered_list):
            if original_idx >= len(original_items):
                continue

            item = original_items[original_idx]
            parts = [f"{i}."]

            # é …ç›®ã®ä¸»è¦æƒ…å ±ã‚’æ§‹ç¯‰
            if "meta_intent" in item:
                parts.append(f"ã€æ„å›³ã€‘{item.get('meta_intent', 'ï¼ˆæœªå®šç¾©ï¼‰')}")
            elif "super_intent" in item:
                parts.append(f"ã€æ„å›³ã€‘{item.get('super_intent', 'ï¼ˆæœªå®šç¾©ï¼‰')}")
            elif "ultra_intent" in item:
                parts.append(f"ã€æ„å›³ã€‘{item.get('ultra_intent', 'ï¼ˆæœªå®šç¾©ï¼‰')}")
            else:
                intent_text = (
                    item.get("intent") or item.get("description") or "ï¼ˆæœªå®šç¾©ï¼‰"
                )
                parts.append(f"ã€æ„å›³ã€‘{intent_text}")

            # objective_facts
            if item.get("objective_facts"):
                parts.append(f"ã€å®¢è¦³çš„äº‹å®Ÿã€‘{item['objective_facts']}")

            # context
            if item.get("context"):
                parts.append(f"ã€èƒŒæ™¯ã€‘{item['context']}")

            uncovered_item_texts.append(" ".join(parts))

        uncovered_items_text = "\n\n".join(uncovered_item_texts)
        max_index = len(uncovered_list) - 1

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
        prompt_text = reassignment_template.format(
            existing_groups=existing_groups_summary,
            uncovered_items=uncovered_items_text,
            max_index=max_index,
        )

        # APIå‘¼ã³å‡ºã—
        try:
            model = gemini_client.GenerativeModel()
            response = model.generate_content(prompt_text)
            response_text = response.text

        except Exception as e:
            print(f"\nâŒ å†å‰²ã‚ŠæŒ¯ã‚Šã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")
            break

        # ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if save_raw:
            raw_output_dir = OUTPUT_DIR / f"raw_reassignment_{level_name}_responses"
            raw_output_dir.mkdir(exist_ok=True)
            cluster_suffix = (
                f"_cluster_{cluster_id:02d}" if cluster_id is not None else ""
            )
            raw_file = (
                raw_output_dir / f"reassignment{cluster_suffix}_retry{retry_count}.txt"
            )
            with open(raw_file, "w", encoding="utf-8") as f:
                f.write(response_text)

        # JSONã‚’ãƒ‘ãƒ¼ã‚¹
        reassignments = preprocess_extract_json_from_response(response_text)
        if reassignments is None:
            print(f"\nâš ï¸ å†å‰²ã‚ŠæŒ¯ã‚Š {retry_count}å›ç›®ã§JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—")
            break

        # å†å‰²ã‚ŠæŒ¯ã‚Šçµæœã‚’æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã«çµ±åˆ
        newly_covered = set()

        for reassignment in reassignments:
            member_indices = reassignment.get("member_indices", [])

            # æœªã‚«ãƒãƒ¼ãƒªã‚¹ãƒˆå†…ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
            original_indices = [
                uncovered_list[idx]
                for idx in member_indices
                if idx < len(uncovered_list)
            ]

            if not original_indices:
                continue

            # ã‚°ãƒ«ãƒ¼ãƒ—åãŒæ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
            group_name = reassignment.get("group_name", "")
            matched_existing_group = None
            matched_group_name_key = None

            for existing_group in existing_groups:
                current_group_name_key = (
                    "meta_intent"
                    if "meta_intent" in existing_group
                    else "super_intent"
                    if "super_intent" in existing_group
                    else "ultra_intent"
                )
                existing_name = existing_group.get(current_group_name_key, "")
                if group_name in existing_name or existing_name in group_name:
                    matched_existing_group = existing_group
                    matched_group_name_key = current_group_name_key
                    break

            # ã‚°ãƒ«ãƒ¼ãƒ—åã‚­ãƒ¼ã®æ±ºå®šï¼ˆæ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æ¨æ¸¬ï¼‰
            if not matched_group_name_key:
                # æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æ¨æ¸¬
                if existing_groups:
                    first_group = existing_groups[0]
                    matched_group_name_key = (
                        "meta_intent"
                        if "meta_intent" in first_group
                        else "super_intent"
                        if "super_intent" in first_group
                        else "ultra_intent"
                    )
                else:
                    matched_group_name_key = "meta_intent"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

            if matched_existing_group:
                # æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ 
                member_key = (
                    "covered_intent_ids"
                    if "covered_intent_ids" in matched_existing_group
                    else "covered_meta_intent_indices"
                    if "covered_meta_intent_indices" in matched_existing_group
                    else "covered_super_intent_indices"
                    if "covered_super_intent_indices" in matched_existing_group
                    else "member_indices"
                )

                # covered_intent_idsã®å ´åˆã¯ç‰¹æ®Šå‡¦ç†
                if member_key == "covered_intent_ids":
                    for idx in original_indices:
                        if (
                            idx < len(original_items)
                            and "cluster_id" in original_items[idx]
                        ):
                            matched_existing_group[member_key].append(
                                {
                                    "cluster_id": int(
                                        original_items[idx]["cluster_id"]
                                    ),
                                    "intent_index": idx,
                                }
                            )
                else:
                    matched_existing_group[member_key].extend(original_indices)

                newly_covered.update(original_indices)
            else:
                # æ–°ã—ã„ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
                # æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰member_keyã‚’æ¨æ¸¬
                if existing_groups:
                    first_group = existing_groups[0]
                    if "covered_intent_ids" in first_group:
                        member_key = "covered_intent_ids"
                    elif "covered_meta_intent_indices" in first_group:
                        member_key = "covered_meta_intent_indices"
                    elif "covered_super_intent_indices" in first_group:
                        member_key = "covered_super_intent_indices"
                    else:
                        member_key = "member_indices"
                else:
                    # æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ãŒãªã„å ´åˆã¯ã€group_name_keyã‹ã‚‰æ¨æ¸¬
                    member_key = (
                        "covered_intent_ids"
                        if matched_group_name_key == "meta_intent"
                        else "covered_meta_intent_indices"
                        if matched_group_name_key == "super_intent"
                        else "covered_super_intent_indices"
                        if matched_group_name_key == "ultra_intent"
                        else "member_indices"
                    )

                # covered_intent_idsã®å ´åˆã¯ç‰¹æ®Šå‡¦ç†
                if member_key == "covered_intent_ids":
                    member_values = [
                        {
                            "cluster_id": int(original_items[idx]["cluster_id"]),
                            "intent_index": idx,
                        }
                        for idx in original_indices
                        if idx < len(original_items)
                        and "cluster_id" in original_items[idx]
                    ]
                else:
                    member_values = original_indices

                new_group = {
                    matched_group_name_key: group_name,
                    "objective_facts": reassignment.get("objective_facts", ""),
                    "context": reassignment.get("context", ""),
                    member_key: member_values,
                }
                existing_groups.append(new_group)
                newly_covered.update(original_indices)

        # æ¬¡ã®è©¦è¡Œã®ãŸã‚ã«æœªã‚«ãƒãƒ¼ã‚’æ›´æ–°
        current_uncovered -= newly_covered

        if not newly_covered:
            print(
                f"\nâš ï¸ å†å‰²ã‚ŠæŒ¯ã‚Š {retry_count}å›ç›®ã§æ–°ãŸã«ã‚«ãƒãƒ¼ã•ã‚ŒãŸé …ç›®ãŒã‚ã‚Šã¾ã›ã‚“"
            )
            break

        print(f"âœ“ {len(newly_covered)}ä»¶ã‚’å†å‰²ã‚ŠæŒ¯ã‚Šã—ã¾ã—ãŸ")

    if current_uncovered:
        print(
            f"\nâš ï¸ {len(current_uncovered)}ä»¶ã®é …ç›®ãŒæœ€çµ‚çš„ã«ã‚«ãƒãƒ¼ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {sorted(current_uncovered)}"
        )

    return existing_groups


def call_gemini_api_with_postprocess(
    prompt_text: str,
    cluster_id: int,
    message_metadata: Dict[str, Dict],
    save_raw: bool = False,
) -> Optional[List[Dict]]:
    """
    ã€APIå‘¼ã³å‡ºã— + å¾Œå‡¦ç†ã€‘Gemini APIã‚’ä½¿ã£ã¦æ„å›³ã‚’æŠ½å‡º

    Args:
        prompt_text: æ„å›³æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        cluster_id: ã‚¯ãƒ©ã‚¹ã‚¿ID
        message_metadata: msg_id -> {full_path, min_start_timestamp} ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        save_raw: ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã‹

    Returns:
        æŠ½å‡ºã•ã‚ŒãŸæ„å›³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneï¼‰
    """
    # APIå‘¼ã³å‡ºã—ï¼ˆlitellmå´ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ï¼‰
    try:
        model = gemini_client.GenerativeModel()
        response = model.generate_content(prompt_text)
        response_text = response.text

    except Exception as e:
        # ä¸¦åˆ—å®Ÿè¡Œæ™‚ã®ãƒ­ã‚°å‡ºåŠ›ã¯tqdmã®pbar.writeã§ã¯ãªãé€šå¸¸ã®printã‚’ä½¿ç”¨
        print(f"\nâŒ ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")
        raise

    # ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if save_raw:
        raw_output_dir = OUTPUT_DIR / "raw_responses"
        raw_output_dir.mkdir(exist_ok=True)
        raw_file = raw_output_dir / f"cluster_{cluster_id:02d}_raw_response.txt"
        with open(raw_file, "w", encoding="utf-8") as f:
            f.write(response_text)

    # å¾Œå‡¦ç†ã‚’å®Ÿè¡Œ
    intents = postprocess_enrich_and_save_intents(
        response_text, cluster_id, message_metadata
    )

    return intents


def aggregate_intents_with_gemini(
    intents: List[Dict], cluster_id: int, grouping_template: str, save_raw: bool = False
) -> Optional[List[Dict]]:
    """
    ã€ä¸Šä½æ„å›³æŠ½å‡ºã€‘æ—¢å­˜ã®æ„å›³ãƒªã‚¹ãƒˆã‹ã‚‰ä¸Šä½æ„å›³ã‚’æŠ½å‡º

    Args:
        intents: æ—¢å­˜ã®æ„å›³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
        cluster_id: ã‚¯ãƒ©ã‚¹ã‚¿ID
        grouping_template: æ„å›³ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        save_raw: ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã‹

    Returns:
        ä¸Šä½æ„å›³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneï¼‰
    """
    if not intents:
        return None

    # LLMã«ã¯æ„å›³ã®å…¨ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ï¼ˆIDç³»ä»¥å¤–ï¼‰ã‚’æ¸¡ã™
    intent_texts = []
    excluded_keys = {"source_message_ids", "cluster_id"}

    for i, intent in enumerate(intents):
        # æ„å›³ã®ä¸»è¦æƒ…å ±ã‚’æ§‹ç¯‰
        parts = [f"{i}."]

        # intent ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆå¿…é ˆï¼‰
        intent_text = intent.get("intent") or intent.get("description") or "ï¼ˆæœªå®šç¾©ï¼‰"
        parts.append(f"ã€æ„å›³ã€‘{intent_text}")

        # ãã®ä»–ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’è¿½åŠ 
        for key, value in intent.items():
            if key in excluded_keys or key in ("intent", "description"):
                continue

            if value:  # å€¤ãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
                if isinstance(value, list):
                    if value:  # ç©ºãƒªã‚¹ãƒˆã§ãªã„å ´åˆ
                        parts.append(f"ã€{key}ã€‘{', '.join(str(v) for v in value)}")
                else:
                    parts.append(f"ã€{key}ã€‘{value}")

        intent_texts.append(" ".join(parts))

    intent_list = "\n\n".join(intent_texts)
    max_index = len(intents) - 1

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«å€¤ã‚’åŸ‹ã‚è¾¼ã¿
    prompt_text = grouping_template.format(intent_list=intent_list, max_index=max_index)

    # APIå‘¼ã³å‡ºã—ï¼ˆlitellmå´ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ï¼‰
    try:
        model = gemini_client.GenerativeModel()
        response = model.generate_content(prompt_text)
        response_text = response.text

    except Exception as e:
        print(
            f"\nâŒ ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ã®ä¸Šä½æ„å›³æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}"
        )
        return None

    # ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if save_raw:
        raw_output_dir = OUTPUT_DIR / "raw_aggregation_responses"
        raw_output_dir.mkdir(exist_ok=True)
        raw_file = raw_output_dir / f"cluster_{cluster_id:02d}_aggregation_raw.txt"
        with open(raw_file, "w", encoding="utf-8") as f:
            f.write(response_text)

    # JSONã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆLLMã‹ã‚‰ã®ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ï¼‰
    groups = preprocess_extract_json_from_response(response_text)
    if groups is None:
        print(f"\nâš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ã®æ„å›³ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã§JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—")
        return None

    # Pythonã§ç¶²ç¾…æ€§ã¨é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯
    covered_indices = set()
    for group in groups:
        member_indices = group.get("member_indices", [])
        covered_indices.update(member_indices)

    all_indices = set(range(len(intents)))
    uncovered = all_indices - covered_indices

    if uncovered:
        print(
            f"\nâš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãŒå…¨ã¦ã®å€‹åˆ¥æ„å›³ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã›ã‚“"
        )
        print(f"   ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {sorted(uncovered)}")
        print(
            f"   Total: {len(intents)}, Covered: {len(covered_indices)}, Uncovered: {len(uncovered)}"
        )

        # å†å‰²ã‚ŠæŒ¯ã‚Šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
        try:
            reassignment_template = load_reassignment_template()

            # æœªã‚«ãƒãƒ¼ã®æ„å›³ã‚’å†å‰²ã‚ŠæŒ¯ã‚Š
            groups = reassign_uncovered_items(
                existing_groups=groups,
                uncovered_indices=uncovered,
                original_items=intents,
                reassignment_template=reassignment_template,
                cluster_id=cluster_id,
                save_raw=save_raw,
                level_name="meta",
                max_retries=3,
            )

            # å†å‰²ã‚ŠæŒ¯ã‚Šå¾Œã€covered_indicesã‚’å†è¨ˆç®—
            covered_indices = set()
            for group in groups:
                member_indices = group.get("member_indices", [])
                covered_indices.update(member_indices)

            uncovered = all_indices - covered_indices
            if not uncovered:
                print(
                    f"âœ“ ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ã®å…¨ã¦ã®å€‹åˆ¥æ„å›³ãŒå†å‰²ã‚ŠæŒ¯ã‚Šã§ã‚«ãƒãƒ¼ã•ã‚Œã¾ã—ãŸ"
                )

        except Exception as e:
            print(f"\nâš ï¸ å†å‰²ã‚ŠæŒ¯ã‚Šå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    duplicate_check = []
    for group in groups:
        duplicate_check.extend(group.get("member_indices", []))

    if len(duplicate_check) != len(set(duplicate_check)):
        duplicates = [idx for idx in duplicate_check if duplicate_check.count(idx) > 1]
        duplicate_set = set(duplicates)
        print(
            f"\nâš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ã§é‡è¤‡ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {duplicate_set}"
        )

        # é‡è¤‡é …ç›®ã‚’ã™ã¹ã¦ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰å‰Šé™¤
        for group in groups:
            original_indices = group.get("member_indices", [])
            group["member_indices"] = [
                idx for idx in original_indices if idx not in duplicate_set
            ]

        # å†å‰²ã‚ŠæŒ¯ã‚Šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§å†åˆ¤å®š
        try:
            reassignment_template = load_reassignment_template()

            # é‡è¤‡é …ç›®ã‚’å†å‰²ã‚ŠæŒ¯ã‚Š
            groups = reassign_uncovered_items(
                existing_groups=groups,
                uncovered_indices=duplicate_set,
                original_items=intents,
                reassignment_template=reassignment_template,
                cluster_id=cluster_id,
                save_raw=save_raw,
                level_name="meta_duplicate",
                max_retries=3,
            )

            print("âœ“ é‡è¤‡é …ç›®ã‚’å†å‰²ã‚ŠæŒ¯ã‚Šã—ã¾ã—ãŸ")

        except Exception as e:
            print(f"\nâš ï¸ é‡è¤‡é …ç›®ã®å†å‰²ã‚ŠæŒ¯ã‚Šå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")

    # Pythonã§meta_intentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰
    meta_intents = []
    for group in groups:
        member_indices = group.get("member_indices", [])

        # statusã‚’æ±ºå®šï¼ˆæœ€ã‚‚é€²ã‚“ã§ã„ãªã„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼‰
        statuses = [
            intents[idx].get("status", "idea")
            for idx in member_indices
            if idx < len(intents)
        ]
        status_priority = {"idea": 0, "todo": 1, "doing": 2, "done": 3}
        aggregate_status = (
            min(statuses, key=lambda s: status_priority.get(s, 0))
            if statuses
            else "idea"
        )

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¸€æ„ãªå€‹åˆ¥æ„å›³IDãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
        covered_intent_ids = [
            {"cluster_id": int(cluster_id), "intent_index": idx}
            for idx in member_indices
            if idx < len(intents)
        ]

        # source_full_paths ã‚’é›†ç´„ï¼ˆå…¨å€‹åˆ¥æ„å›³ã‹ã‚‰åé›†ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼‰
        aggregated_full_paths = []
        for idx in member_indices:
            if idx < len(intents):
                paths = intents[idx].get("source_full_paths", [])
                aggregated_full_paths.extend(paths)
        aggregated_full_paths = sorted(set(aggregated_full_paths))

        # min_start_timestamp ã‚’é›†ç´„ï¼ˆå…¨å€‹åˆ¥æ„å›³ã‹ã‚‰æœ€å°å€¤ã‚’å–å¾—ï¼‰
        timestamps = []
        for idx in member_indices:
            if idx < len(intents):
                ts = intents[idx].get("min_start_timestamp")
                if ts:
                    timestamps.append(ts)
        aggregated_min_timestamp = min(timestamps) if timestamps else None

        meta_intent = {
            "meta_intent": group.get("group_name", "ï¼ˆæœªå®šç¾©ï¼‰"),
            "objective_facts": group.get("objective_facts", ""),
            "context": group.get("context", ""),
            "covered_intent_ids": covered_intent_ids,
            "source_full_paths": aggregated_full_paths,
            "min_start_timestamp": aggregated_min_timestamp,
            "aggregate_status": aggregate_status,
        }
        meta_intents.append(meta_intent)

    # ä¸Šä½æ„å›³ã‚’ä¿å­˜
    output_file = AGGREGATED_DIR / f"cluster_{cluster_id:02d}_aggregated.json"
    aggregation_result = {
        "cluster_id": int(cluster_id),
        "original_intents_count": len(intents),
        "meta_intents": meta_intents,
        "original_intents": intents,
        "validation": {
            "total_intents": len(intents),
            "covered_intents": len(covered_indices),
            "uncovered_intents": len(uncovered),
            "uncovered_indices": sorted(uncovered) if uncovered else [],
            "has_duplicates": len(duplicate_check) != len(set(duplicate_check)),
        },
    }

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(aggregation_result, f, ensure_ascii=False, indent=2)

    return meta_intents


def collect_all_meta_intents(cluster_ids: List[int]) -> tuple[List[Dict], Dict]:
    """
    å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸Šä½æ„å›³ã‚’åé›†

    Args:
        cluster_ids: å‡¦ç†æ¸ˆã¿ã‚¯ãƒ©ã‚¹ã‚¿IDã®ãƒªã‚¹ãƒˆ

    Returns:
        (å…¨ã¦ã®ä¸Šä½æ„å›³ã®ãƒªã‚¹ãƒˆ, çµ±è¨ˆæƒ…å ±)
        çµ±è¨ˆæƒ…å ±ã«ã¯ total_individual_intents ãŒå«ã¾ã‚Œã‚‹
    """
    all_meta_intents = []
    total_individual_intents = 0

    for cluster_id in cluster_ids:
        aggregated_file = AGGREGATED_DIR / f"cluster_{cluster_id:02d}_aggregated.json"
        if not aggregated_file.exists():
            print(
                f"âš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} ã®ä¸Šä½æ„å›³ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {aggregated_file}"
            )
            continue

        with open(aggregated_file, "r", encoding="utf-8") as f:
            data = json.load(f)

        meta_intents = data.get("meta_intents", [])
        for meta_intent in meta_intents:
            # ã‚¯ãƒ©ã‚¹ã‚¿IDã‚’è¿½åŠ 
            meta_intent_with_cluster = meta_intent.copy()
            meta_intent_with_cluster["source_cluster_id"] = int(cluster_id)

            # source_full_pathsã‚’å®‰å®šã—ãŸé †åºã«ã‚½ãƒ¼ãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡å‘ä¸Šï¼‰
            if "source_full_paths" in meta_intent_with_cluster:
                meta_intent_with_cluster["source_full_paths"] = sorted(
                    meta_intent_with_cluster["source_full_paths"]
                )

            all_meta_intents.append(meta_intent_with_cluster)

        # å€‹åˆ¥æ„å›³ã®ç·æ•°ã‚’é›†è¨ˆ
        total_individual_intents += data.get("original_intents_count", 0)

    # meta_intentsã‚’å®‰å®šã—ãŸé †åºã«ã‚½ãƒ¼ãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡å‘ä¸Šï¼‰
    # 1. source_cluster_idï¼ˆã‚¯ãƒ©ã‚¹ã‚¿IDï¼‰ã§ã‚½ãƒ¼ãƒˆ
    # 2. min_start_timestampï¼ˆæ™‚ç³»åˆ—ï¼‰ã§ã‚½ãƒ¼ãƒˆ
    all_meta_intents.sort(
        key=lambda x: (x.get("source_cluster_id", 0), x.get("min_start_timestamp", ""))
    )

    stats = {"total_individual_intents": total_individual_intents}

    return all_meta_intents, stats


def aggregate_cross_cluster_intents(
    meta_intents: List[Dict],
    grouping_template: str,
    total_individual_intents: int,
    save_raw: bool = False,
) -> Optional[List[Dict]]:
    """
    ã€ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³æŠ½å‡ºã€‘å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸Šä½æ„å›³ã‹ã‚‰ã•ã‚‰ã«ä¸Šä½ã®æ„å›³ã‚’æŠ½å‡º

    Args:
        meta_intents: å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸Šä½æ„å›³ã®ãƒªã‚¹ãƒˆ
        grouping_template: æ„å›³ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        total_individual_intents: å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã®å€‹åˆ¥æ„å›³ã®ç·æ•°
        save_raw: ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã‹

    Returns:
        ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneï¼‰
    """
    if not meta_intents:
        return None

    # LLMã«ã¯ç°¡æ½”ãªæƒ…å ±ã®ã¿æ¸¡ã™
    intent_texts = []

    for i, meta in enumerate(meta_intents):
        # æ„å›³ã®ä¸»è¦æƒ…å ±ã‚’æ§‹ç¯‰
        parts = [f"{i}."]

        # meta_intentæœ¬æ–‡ï¼ˆå¿…é ˆï¼‰
        meta_text = meta.get("meta_intent") or "ï¼ˆæœªå®šç¾©ï¼‰"
        parts.append(f"ã€æ„å›³ã€‘{meta_text}")

        # objective_factsï¼ˆå®¢è¦³çš„äº‹å®Ÿï¼‰
        if meta.get("objective_facts"):
            parts.append(f"ã€å®¢è¦³çš„äº‹å®Ÿã€‘{meta['objective_facts']}")

        # contextï¼ˆèƒŒæ™¯ï¼‰
        if meta.get("context"):
            parts.append(f"ã€èƒŒæ™¯ã€‘{meta['context']}")

        # source_full_pathsï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¤æ–­ã«å¿…è¦ï¼‰
        if meta.get("source_full_paths"):
            paths = ", ".join(meta["source_full_paths"])
            parts.append(f"ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‘{paths}")

        # aggregate_status
        if meta.get("aggregate_status"):
            parts.append(f"ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€‘{meta['aggregate_status']}")

        intent_texts.append(" ".join(parts))

    intent_list = "\n\n".join(intent_texts)
    max_index = len(meta_intents) - 1

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«å€¤ã‚’åŸ‹ã‚è¾¼ã¿
    prompt_text = grouping_template.format(intent_list=intent_list, max_index=max_index)

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜
    prompt_output_file = CROSS_CLUSTER_DIR / "cross_cluster_prompt.md"
    with open(prompt_output_file, "w", encoding="utf-8") as f:
        f.write(prompt_text)

    # APIå‘¼ã³å‡ºã—ï¼ˆlitellmå´ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ï¼‰
    try:
        model = gemini_client.GenerativeModel()
        response = model.generate_content(prompt_text)
        response_text = response.text

    except Exception as e:
        print(f"\nâŒ ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")
        return None

    # ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if save_raw:
        raw_output_dir = OUTPUT_DIR / "raw_cross_cluster_responses"
        raw_output_dir.mkdir(exist_ok=True)
        raw_file = raw_output_dir / "cross_cluster_aggregation_raw.txt"
        with open(raw_file, "w", encoding="utf-8") as f:
            f.write(response_text)

    # JSONã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆLLMã‹ã‚‰ã®ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ï¼‰
    groups = preprocess_extract_json_from_response(response_text)
    if groups is None:
        print("\nâš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­æ„å›³ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã§JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—")
        return None

    # Pythonã§ç¶²ç¾…æ€§ã¨é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯
    covered_indices = set()
    for group in groups:
        member_indices = group.get("member_indices", [])
        covered_indices.update(member_indices)

    all_indices = set(range(len(meta_intents)))
    uncovered = all_indices - covered_indices

    if uncovered:
        print("\nâš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãŒå…¨ã¦ã®meta_intentã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã›ã‚“")
        print(f"   ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {sorted(uncovered)}")
        print(
            f"   Total: {len(meta_intents)}, Covered: {len(covered_indices)}, Uncovered: {len(uncovered)}"
        )

        # å†å‰²ã‚ŠæŒ¯ã‚Šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
        try:
            reassignment_template = load_reassignment_template()

            # æœªã‚«ãƒãƒ¼ã®meta_intentã‚’å†å‰²ã‚ŠæŒ¯ã‚Š
            groups = reassign_uncovered_items(
                existing_groups=groups,
                uncovered_indices=uncovered,
                original_items=meta_intents,
                reassignment_template=reassignment_template,
                cluster_id=None,  # ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ãªã®ã§None
                save_raw=save_raw,
                level_name="super",
                max_retries=3,
            )

            # å†å‰²ã‚ŠæŒ¯ã‚Šå¾Œã€covered_indicesã‚’å†è¨ˆç®—
            covered_indices = set()
            for group in groups:
                member_indices = group.get("member_indices", [])
                covered_indices.update(member_indices)

            uncovered = all_indices - covered_indices
            if not uncovered:
                print("âœ“ å…¨ã¦ã®meta_intentãŒå†å‰²ã‚ŠæŒ¯ã‚Šã§ã‚«ãƒãƒ¼ã•ã‚Œã¾ã—ãŸ")

        except Exception as e:
            print(f"\nâš ï¸ å†å‰²ã‚ŠæŒ¯ã‚Šå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    duplicate_check = []
    for group in groups:
        duplicate_check.extend(group.get("member_indices", []))

    if len(duplicate_check) != len(set(duplicate_check)):
        duplicates = [idx for idx in duplicate_check if duplicate_check.count(idx) > 1]
        duplicate_set = set(duplicates)
        print(
            f"\nâš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã§é‡è¤‡ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {duplicate_set}"
        )

        # é‡è¤‡é …ç›®ã‚’ã™ã¹ã¦ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰å‰Šé™¤
        for group in groups:
            original_indices = group.get("member_indices", [])
            group["member_indices"] = [
                idx for idx in original_indices if idx not in duplicate_set
            ]

        # å†å‰²ã‚ŠæŒ¯ã‚Šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§å†åˆ¤å®š
        try:
            reassignment_template = load_reassignment_template()

            # é‡è¤‡é …ç›®ã‚’å†å‰²ã‚ŠæŒ¯ã‚Š
            groups = reassign_uncovered_items(
                existing_groups=groups,
                uncovered_indices=duplicate_set,
                original_items=meta_intents,
                reassignment_template=reassignment_template,
                cluster_id=None,  # ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ãªã®ã§None
                save_raw=save_raw,
                level_name="super_duplicate",
                max_retries=3,
            )

            print("âœ“ é‡è¤‡é …ç›®ã‚’å†å‰²ã‚ŠæŒ¯ã‚Šã—ã¾ã—ãŸ")

        except Exception as e:
            print(f"\nâš ï¸ é‡è¤‡é …ç›®ã®å†å‰²ã‚ŠæŒ¯ã‚Šå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")

    # Pythonã§super_intentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰
    super_intents = []
    for group in groups:
        member_indices = group.get("member_indices", [])

        # statusã‚’æ±ºå®šï¼ˆæœ€ã‚‚é€²ã‚“ã§ã„ãªã„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼‰
        statuses = [
            meta_intents[idx].get("aggregate_status", "idea")
            for idx in member_indices
            if idx < len(meta_intents)
        ]
        status_priority = {"idea": 0, "todo": 1, "doing": 2, "done": 3}
        aggregate_status = (
            min(statuses, key=lambda s: status_priority.get(s, 0))
            if statuses
            else "idea"
        )

        # meta_intentã‚’é€šã˜ã¦å€‹åˆ¥æ„å›³ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«IDã‚’flatten
        covered_intent_ids_flat = []
        for meta_idx in member_indices:
            if meta_idx < len(meta_intents):
                meta_intent = meta_intents[meta_idx]
                covered_intent_ids_flat.extend(
                    meta_intent.get("covered_intent_ids", [])
                )

        # é‡è¤‡ã‚’é™¤å»ï¼ˆdict ã¯ hashable ã§ãªã„ã®ã§ tuple ã§ä¸€æ„åŒ–ï¼‰
        unique_ids = []
        seen = set()
        for intent_id in covered_intent_ids_flat:
            key = (intent_id["cluster_id"], intent_id["intent_index"])
            if key not in seen:
                seen.add(key)
                unique_ids.append(intent_id)

        # cluster_id, intent_index ã§ã‚½ãƒ¼ãƒˆ
        covered_intent_ids_flat = sorted(
            unique_ids, key=lambda x: (x["cluster_id"], x["intent_index"])
        )

        # source_full_paths ã‚’é›†ç´„ï¼ˆå…¨meta_intentã‹ã‚‰åé›†ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼‰
        aggregated_full_paths = []
        for meta_idx in member_indices:
            if meta_idx < len(meta_intents):
                paths = meta_intents[meta_idx].get("source_full_paths", [])
                aggregated_full_paths.extend(paths)
        aggregated_full_paths = sorted(set(aggregated_full_paths))

        # min_start_timestamp ã‚’é›†ç´„ï¼ˆå…¨meta_intentã‹ã‚‰æœ€å°å€¤ã‚’å–å¾—ï¼‰
        timestamps = []
        for meta_idx in member_indices:
            if meta_idx < len(meta_intents):
                ts = meta_intents[meta_idx].get("min_start_timestamp")
                if ts:
                    timestamps.append(ts)
        aggregated_min_timestamp = min(timestamps) if timestamps else None

        super_intent = {
            "super_intent": group.get("group_name", "ï¼ˆæœªå®šç¾©ï¼‰"),
            "objective_facts": group.get("objective_facts", ""),
            "context": group.get("context", ""),
            "covered_meta_intent_indices": member_indices,
            "covered_intent_ids_flat": covered_intent_ids_flat,
            "source_full_paths": aggregated_full_paths,
            "min_start_timestamp": aggregated_min_timestamp,
            "aggregate_status": aggregate_status,
        }
        super_intents.append(super_intent)

    # flatten ã•ã‚ŒãŸå€‹åˆ¥æ„å›³IDã®ç¶²ç¾…æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«IDãƒ™ãƒ¼ã‚¹ï¼‰
    covered_flat_ids = set()
    for super_intent in super_intents:
        for intent_id in super_intent["covered_intent_ids_flat"]:
            covered_flat_ids.add((intent_id["cluster_id"], intent_id["intent_index"]))

    # å…¨meta_intentsã«å«ã¾ã‚Œã‚‹å€‹åˆ¥æ„å›³IDã‚’åé›†
    all_individual_intent_ids = set()
    for meta_intent in meta_intents:
        for intent_id in meta_intent.get("covered_intent_ids", []):
            all_individual_intent_ids.add(
                (intent_id["cluster_id"], intent_id["intent_index"])
            )

    uncovered_flat = all_individual_intent_ids - covered_flat_ids

    if uncovered_flat:
        print(
            "\nâš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆflattenï¼‰ãŒå…¨ã¦ã®å€‹åˆ¥æ„å›³ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã›ã‚“"
        )
        print(f"   ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„å€‹åˆ¥æ„å›³ID: {sorted(uncovered_flat)}")
        print(
            f"   Total: {len(all_individual_intent_ids)}, Covered: {len(covered_flat_ids)}, Uncovered: {len(uncovered_flat)}"
        )

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    duplicate_flat_ids = []
    for super_intent in super_intents:
        for intent_id in super_intent["covered_intent_ids_flat"]:
            duplicate_flat_ids.append(
                (intent_id["cluster_id"], intent_id["intent_index"])
            )

    if len(duplicate_flat_ids) != len(set(duplicate_flat_ids)):
        dup_set = [x for x in duplicate_flat_ids if duplicate_flat_ids.count(x) > 1]
        print(
            f"\nâš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆflattenï¼‰ã§é‡è¤‡ã™ã‚‹å€‹åˆ¥æ„å›³IDãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {set(dup_set)}"
        )

    # ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³ã‚’ä¿å­˜
    output_file = CROSS_CLUSTER_DIR / "super_intents.json"
    cross_cluster_result = {
        "generated_at": datetime.now().isoformat(),
        "total_meta_intents": len(meta_intents),
        "total_individual_intents": total_individual_intents,
        "super_intents": super_intents,
        "meta_intents": meta_intents,
        "validation": {
            "meta_level": {
                "total_meta_intents": len(meta_intents),
                "covered_meta_intents": len(covered_indices),
                "uncovered_meta_intents": len(uncovered),
                "uncovered_meta_indices": sorted(uncovered) if uncovered else [],
                "has_duplicates": len(duplicate_check) != len(set(duplicate_check)),
            },
            "individual_level": {
                "total_individual_intents": len(all_individual_intent_ids),
                "covered_individual_intents": len(covered_flat_ids),
                "uncovered_individual_intents": len(uncovered_flat),
                "uncovered_individual_ids": [
                    {"cluster_id": cid, "intent_index": idx}
                    for cid, idx in sorted(uncovered_flat)
                ],
                "has_duplicates": len(duplicate_flat_ids)
                != len(set(duplicate_flat_ids)),
            },
        },
    }

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(cross_cluster_result, f, ensure_ascii=False, indent=2)

    # è©³ç´°å±•é–‹ç‰ˆã‚‚ç”Ÿæˆï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ã¯å¤‰æ›´ã—ãªã„ï¼‰
    enrich_and_save_super_intents(cross_cluster_result, output_file.parent)

    return super_intents, meta_intents, total_individual_intents


def load_all_intents_for_enrichment() -> Dict[tuple, Dict]:
    """
    å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã®å€‹åˆ¥ intent ã‚’èª­ã¿è¾¼ã¿ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«IDã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–

    Returns:
        (cluster_id, intent_index) -> intentè©³ç´° ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    """
    all_intents = {}

    # å…¨ã¦ã® processed ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    for processed_file in sorted(PROCESSED_DIR.glob("cluster_*_processed.json")):
        with open(processed_file, "r", encoding="utf-8") as f:
            intents = json.load(f)

        for idx, intent in enumerate(intents):
            cluster_id = intent.get("cluster_id")
            if cluster_id is not None:
                key = (cluster_id, idx)
                all_intents[key] = intent

    return all_intents


def enrich_intents_with_details(
    intents_list: List[Dict],
    all_intents: Dict[tuple, Dict],
    intent_id_key: str = "covered_intent_ids_flat",
) -> List[Dict]:
    """
    æ„å›³ãƒªã‚¹ãƒˆã«å€‹åˆ¥ intent ã®è©³ç´°ã‚’å±•é–‹ï¼ˆã‚³ãƒ”ãƒ¼ã‚’ä½œæˆã—ã¦å¤‰æ›´ï¼‰

    Args:
        intents_list: super_intents ã¾ãŸã¯ ultra_intents ã®ãƒªã‚¹ãƒˆ
        all_intents: (cluster_id, intent_index) -> intentè©³ç´° ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        intent_id_key: å€‹åˆ¥æ„å›³IDã®ã‚­ãƒ¼å

    Returns:
        è©³ç´°å±•é–‹ã•ã‚ŒãŸæ„å›³ãƒªã‚¹ãƒˆï¼ˆæ–°è¦ã‚³ãƒ”ãƒ¼ï¼‰
    """
    enriched_list = []

    for intent in intents_list:
        # ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã—ãªã„ï¼‰
        enriched_intent = intent.copy()
        covered_ids = intent.get(intent_id_key, [])

        # å€‹åˆ¥ intent ã®è©³ç´°ã‚’åé›†
        covered_intents_details = []
        missing_ids = []

        for intent_id in covered_ids:
            cluster_id = intent_id["cluster_id"]
            intent_index = intent_id["intent_index"]
            key = (cluster_id, intent_index)

            intent_detail = all_intents.get(key)
            if intent_detail:
                covered_intents_details.append(intent_detail)
            else:
                missing_ids.append(intent_id)

        # è©³ç´°æƒ…å ±ã‚’è¿½åŠ ï¼ˆã‚³ãƒ”ãƒ¼ã«å¯¾ã—ã¦ï¼‰
        enriched_intent["covered_intents_details"] = covered_intents_details

        # çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
        enriched_intent["_stats"] = {
            "total_covered": len(covered_ids),
            "resolved": len(covered_intents_details),
            "missing": len(missing_ids),
        }

        if missing_ids:
            enriched_intent["_missing_ids"] = missing_ids

        enriched_list.append(enriched_intent)

    return enriched_list


def save_enriched_intents(
    original_result: Dict,
    enriched_key: str,
    enriched_intents: List[Dict],
    output_file: Path,
    level_name: str = "ultra",
):
    """
    è©³ç´°å±•é–‹ç‰ˆã®æ„å›³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜

    Args:
        original_result: å…ƒã®çµæœãƒ‡ãƒ¼ã‚¿
        enriched_key: æ„å›³ãƒªã‚¹ãƒˆã®ã‚­ãƒ¼åï¼ˆ"ultra_intents" or "super_intents"ï¼‰
        enriched_intents: è©³ç´°å±•é–‹ã•ã‚ŒãŸæ„å›³ãƒªã‚¹ãƒˆ
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        level_name: ãƒ¬ãƒ™ãƒ«åï¼ˆãƒ­ã‚°è¡¨ç¤ºç”¨ï¼‰
    """
    # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦è©³ç´°å±•é–‹ç‰ˆã‚’ä½œæˆ
    enriched_result = original_result.copy()
    enriched_result[enriched_key] = enriched_intents

    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(enriched_result, f, ensure_ascii=False, indent=2)

    print(f"âœ“ è©³ç´°å±•é–‹ç‰ˆã‚’ä¿å­˜: {output_file}")

    # çµ±è¨ˆè¡¨ç¤º
    for i, intent in enumerate(enriched_intents, 1):
        stats = intent["_stats"]
        intent_name = intent.get(f"{level_name}_intent", "ï¼ˆæœªå®šç¾©ï¼‰")
        print(f"  {i}. {intent_name}: {stats['total_covered']}ä»¶ã®å€‹åˆ¥æ„å›³")
        if stats["missing"] > 0:
            print(f"     âš ï¸ æœªè§£æ±º: {stats['missing']}ä»¶")


def enrich_and_save_ultra_intents(ultra_result: Dict, output_dir: Path):
    """
    ultra_intents ã«å€‹åˆ¥ intent ã®è©³ç´°ã‚’å±•é–‹ã—ã¦åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        ultra_result: ultra_intents.json ã®å†…å®¹ï¼ˆå¤‰æ›´ã•ã‚Œãªã„ï¼‰
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    # å…¨å€‹åˆ¥ intent ã‚’èª­ã¿è¾¼ã¿
    print("\nå€‹åˆ¥ intent ã®è©³ç´°ã‚’å±•é–‹ä¸­...")
    all_intents = load_all_intents_for_enrichment()
    print(f"âœ“ {len(all_intents)}ä»¶ã®å€‹åˆ¥ intent ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # ultra_intents ã‚’è©³ç´°å±•é–‹ï¼ˆã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼‰
    ultra_intents = ultra_result.get("ultra_intents", [])
    enriched_ultra_intents = enrich_intents_with_details(
        ultra_intents, all_intents, "covered_intent_ids_flat"
    )

    # è©³ç´°å±•é–‹ç‰ˆã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    enriched_file = output_dir / "ultra_intents_enriched.json"
    save_enriched_intents(
        ultra_result, "ultra_intents", enriched_ultra_intents, enriched_file, "ultra"
    )


def enrich_and_save_super_intents(super_result: Dict, output_dir: Path):
    """
    super_intents ã«å€‹åˆ¥ intent ã®è©³ç´°ã‚’å±•é–‹ã—ã¦åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        super_result: super_intents.json ã®å†…å®¹ï¼ˆå¤‰æ›´ã•ã‚Œãªã„ï¼‰
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    # å…¨å€‹åˆ¥ intent ã‚’èª­ã¿è¾¼ã¿
    print("\nå€‹åˆ¥ intent ã®è©³ç´°ã‚’å±•é–‹ä¸­...")
    all_intents = load_all_intents_for_enrichment()
    print(f"âœ“ {len(all_intents)}ä»¶ã®å€‹åˆ¥ intent ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # super_intents ã‚’è©³ç´°å±•é–‹ï¼ˆã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼‰
    super_intents = super_result.get("super_intents", [])
    enriched_super_intents = enrich_intents_with_details(
        super_intents, all_intents, "covered_intent_ids_flat"
    )

    # è©³ç´°å±•é–‹ç‰ˆã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    enriched_file = output_dir / "super_intents_enriched.json"
    save_enriched_intents(
        super_result, "super_intents", enriched_super_intents, enriched_file, "super"
    )


def aggregate_super_intents_recursively(
    super_intents: List[Dict],
    meta_intents: List[Dict],
    total_individual_intents: int,
    grouping_template: str,
    save_raw: bool = False,
) -> Optional[List[Dict]]:
    """
    ã€2æ®µéšç›®ã®æŠ½è±¡åŒ–ã€‘super_intentsãŒ50ä»¶ä»¥ä¸Šã®å ´åˆã€ã•ã‚‰ã«æŠ½è±¡åŒ–

    Args:
        super_intents: 1æ®µéšç›®ã®super_intentsãƒªã‚¹ãƒˆ
        meta_intents: å…ƒã®meta_intentsãƒªã‚¹ãƒˆï¼ˆå‚ç…§ç”¨ï¼‰
        total_individual_intents: å€‹åˆ¥æ„å›³ã®ç·æ•°
        grouping_template: æ„å›³ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        save_raw: ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã‹

    Returns:
        æœ€çµ‚çš„ãªä¸Šä½æ„å›³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneï¼‰
    """
    if not super_intents or len(super_intents) < 10:
        return None

    print(
        f"\nğŸ“Š super_intentsãŒ{len(super_intents)}ä»¶ã‚ã‚‹ãŸã‚ã€ã•ã‚‰ã«æŠ½è±¡åŒ–ã‚’å®Ÿè¡Œã—ã¾ã™"
    )

    # LLMã«ã¯ç°¡æ½”ãªæƒ…å ±ã®ã¿æ¸¡ã™ï¼ˆmeta_intentsã¨åŒã˜å½¢å¼ï¼‰
    intent_texts = []

    for i, super_intent in enumerate(super_intents):
        # æ„å›³ã®ä¸»è¦æƒ…å ±ã‚’æ§‹ç¯‰
        parts = [f"{i}."]

        # super_intentæœ¬æ–‡ï¼ˆå¿…é ˆï¼‰
        super_text = super_intent.get("super_intent") or "ï¼ˆæœªå®šç¾©ï¼‰"
        parts.append(f"ã€æ„å›³ã€‘{super_text}")

        # objective_factsï¼ˆå®¢è¦³çš„äº‹å®Ÿï¼‰
        if super_intent.get("objective_facts"):
            parts.append(f"ã€å®¢è¦³çš„äº‹å®Ÿã€‘{super_intent['objective_facts']}")

        # contextï¼ˆèƒŒæ™¯ï¼‰
        if super_intent.get("context"):
            parts.append(f"ã€èƒŒæ™¯ã€‘{super_intent['context']}")

        # source_full_pathsï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¤æ–­ã«å¿…è¦ï¼‰
        if super_intent.get("source_full_paths"):
            paths = ", ".join(super_intent["source_full_paths"])
            parts.append(f"ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‘{paths}")

        # aggregate_status
        if super_intent.get("aggregate_status"):
            parts.append(f"ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€‘{super_intent['aggregate_status']}")

        intent_texts.append(" ".join(parts))

    intent_list = "\n\n".join(intent_texts)
    max_index = len(super_intents) - 1

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«å€¤ã‚’åŸ‹ã‚è¾¼ã¿
    prompt_text = grouping_template.format(intent_list=intent_list, max_index=max_index)

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜
    prompt_output_file = CROSS_CLUSTER_DIR / "ultra_intent_prompt.md"
    with open(prompt_output_file, "w", encoding="utf-8") as f:
        f.write(prompt_text)

    # APIå‘¼ã³å‡ºã—ï¼ˆlitellmå´ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ï¼‰
    try:
        model = gemini_client.GenerativeModel()
        response = model.generate_content(prompt_text)
        response_text = response.text

    except Exception as e:
        print(f"\nâŒ 2æ®µéšç›®ã®æŠ½è±¡åŒ–ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")
        return None

    # ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if save_raw:
        raw_output_dir = OUTPUT_DIR / "raw_ultra_intent_responses"
        raw_output_dir.mkdir(exist_ok=True)
        raw_file = raw_output_dir / "ultra_intent_raw.txt"
        with open(raw_file, "w", encoding="utf-8") as f:
            f.write(response_text)

    # JSONã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆLLMã‹ã‚‰ã®ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ï¼‰
    groups = preprocess_extract_json_from_response(response_text)
    if groups is None:
        print("\nâš ï¸ 2æ®µéšç›®ã®æŠ½è±¡åŒ–ã§JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—")
        return None

    # Pythonã§ç¶²ç¾…æ€§ã¨é‡è¤‡ã‚’ãƒã‚§ãƒƒã‚¯
    covered_indices = set()
    for group in groups:
        member_indices = group.get("member_indices", [])
        covered_indices.update(member_indices)

    all_indices = set(range(len(super_intents)))
    uncovered = all_indices - covered_indices

    if uncovered:
        print("\nâš ï¸ 2æ®µéšç›®ã®æŠ½è±¡åŒ–ãŒå…¨ã¦ã®super_intentã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã›ã‚“")
        print(f"   ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {sorted(uncovered)}")
        print(
            f"   Total: {len(super_intents)}, Covered: {len(covered_indices)}, Uncovered: {len(uncovered)}"
        )

        # å†å‰²ã‚ŠæŒ¯ã‚Šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
        try:
            reassignment_template = load_reassignment_template()

            # æœªã‚«ãƒãƒ¼ã®super_intentã‚’å†å‰²ã‚ŠæŒ¯ã‚Š
            groups = reassign_uncovered_items(
                existing_groups=groups,
                uncovered_indices=uncovered,
                original_items=super_intents,
                reassignment_template=reassignment_template,
                cluster_id=None,  # ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ãªã®ã§None
                save_raw=save_raw,
                level_name="ultra",
                max_retries=3,
            )

            # å†å‰²ã‚ŠæŒ¯ã‚Šå¾Œã€covered_indicesã‚’å†è¨ˆç®—
            covered_indices = set()
            for group in groups:
                member_indices = group.get("member_indices", [])
                covered_indices.update(member_indices)

            uncovered = all_indices - covered_indices
            if not uncovered:
                print("âœ“ å…¨ã¦ã®super_intentãŒå†å‰²ã‚ŠæŒ¯ã‚Šã§ã‚«ãƒãƒ¼ã•ã‚Œã¾ã—ãŸ")

        except Exception as e:
            print(f"\nâš ï¸ å†å‰²ã‚ŠæŒ¯ã‚Šå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    duplicate_check = []
    for group in groups:
        duplicate_check.extend(group.get("member_indices", []))

    if len(duplicate_check) != len(set(duplicate_check)):
        duplicates = [idx for idx in duplicate_check if duplicate_check.count(idx) > 1]
        duplicate_set = set(duplicates)
        print(
            f"\nâš ï¸ 2æ®µéšç›®ã®æŠ½è±¡åŒ–ã§é‡è¤‡ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {duplicate_set}"
        )

        # é‡è¤‡é …ç›®ã‚’ã™ã¹ã¦ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰å‰Šé™¤
        for group in groups:
            original_indices = group.get("member_indices", [])
            group["member_indices"] = [
                idx for idx in original_indices if idx not in duplicate_set
            ]

        # å†å‰²ã‚ŠæŒ¯ã‚Šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§å†åˆ¤å®š
        try:
            reassignment_template = load_reassignment_template()

            # é‡è¤‡é …ç›®ã‚’å†å‰²ã‚ŠæŒ¯ã‚Š
            groups = reassign_uncovered_items(
                existing_groups=groups,
                uncovered_indices=duplicate_set,
                original_items=super_intents,
                reassignment_template=reassignment_template,
                cluster_id=None,  # ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ãªã®ã§None
                save_raw=save_raw,
                level_name="ultra_duplicate",
                max_retries=3,
            )

            print("âœ“ é‡è¤‡é …ç›®ã‚’å†å‰²ã‚ŠæŒ¯ã‚Šã—ã¾ã—ãŸ")

        except Exception as e:
            print(f"\nâš ï¸ é‡è¤‡é …ç›®ã®å†å‰²ã‚ŠæŒ¯ã‚Šå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {type(e).__name__}: {e}")

    # Pythonã§ultra_intentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰
    ultra_intents = []
    for group in groups:
        member_indices = group.get("member_indices", [])

        # statusã‚’æ±ºå®šï¼ˆæœ€ã‚‚é€²ã‚“ã§ã„ãªã„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼‰
        statuses = [
            super_intents[idx].get("aggregate_status", "idea")
            for idx in member_indices
            if idx < len(super_intents)
        ]
        status_priority = {"idea": 0, "todo": 1, "doing": 2, "done": 3}
        aggregate_status = (
            min(statuses, key=lambda s: status_priority.get(s, 0))
            if statuses
            else "idea"
        )

        # super_intentã‚’é€šã˜ã¦å€‹åˆ¥æ„å›³ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«IDã‚’flatten
        covered_intent_ids_flat = []
        for super_idx in member_indices:
            if super_idx < len(super_intents):
                super_intent = super_intents[super_idx]
                covered_intent_ids_flat.extend(
                    super_intent.get("covered_intent_ids_flat", [])
                )

        # é‡è¤‡ã‚’é™¤å»ï¼ˆdict ã¯ hashable ã§ãªã„ã®ã§ tuple ã§ä¸€æ„åŒ–ï¼‰
        unique_ids = []
        seen = set()
        for intent_id in covered_intent_ids_flat:
            key = (intent_id["cluster_id"], intent_id["intent_index"])
            if key not in seen:
                seen.add(key)
                unique_ids.append(intent_id)

        # cluster_id, intent_index ã§ã‚½ãƒ¼ãƒˆ
        covered_intent_ids_flat = sorted(
            unique_ids, key=lambda x: (x["cluster_id"], x["intent_index"])
        )

        # source_full_paths ã‚’é›†ç´„ï¼ˆå…¨super_intentã‹ã‚‰åé›†ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼‰
        aggregated_full_paths = []
        for super_idx in member_indices:
            if super_idx < len(super_intents):
                paths = super_intents[super_idx].get("source_full_paths", [])
                aggregated_full_paths.extend(paths)
        aggregated_full_paths = sorted(set(aggregated_full_paths))

        # min_start_timestamp ã‚’é›†ç´„ï¼ˆå…¨super_intentã‹ã‚‰æœ€å°å€¤ã‚’å–å¾—ï¼‰
        timestamps = []
        for super_idx in member_indices:
            if super_idx < len(super_intents):
                ts = super_intents[super_idx].get("min_start_timestamp")
                if ts:
                    timestamps.append(ts)
        aggregated_min_timestamp = min(timestamps) if timestamps else None

        ultra_intent = {
            "ultra_intent": group.get("group_name", "ï¼ˆæœªå®šç¾©ï¼‰"),
            "objective_facts": group.get("objective_facts", ""),
            "context": group.get("context", ""),
            "covered_super_intent_indices": member_indices,
            "covered_intent_ids_flat": covered_intent_ids_flat,
            "source_full_paths": aggregated_full_paths,
            "min_start_timestamp": aggregated_min_timestamp,
            "aggregate_status": aggregate_status,
        }
        ultra_intents.append(ultra_intent)

    # flatten ã•ã‚ŒãŸå€‹åˆ¥æ„å›³IDã®ç¶²ç¾…æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«IDãƒ™ãƒ¼ã‚¹ï¼‰
    covered_flat_ids = set()
    for ultra_intent in ultra_intents:
        for intent_id in ultra_intent["covered_intent_ids_flat"]:
            covered_flat_ids.add((intent_id["cluster_id"], intent_id["intent_index"]))

    # å…¨super_intentsã«å«ã¾ã‚Œã‚‹å€‹åˆ¥æ„å›³IDã‚’åé›†
    all_individual_intent_ids = set()
    for super_intent in super_intents:
        for intent_id in super_intent.get("covered_intent_ids_flat", []):
            all_individual_intent_ids.add(
                (intent_id["cluster_id"], intent_id["intent_index"])
            )

    uncovered_flat = all_individual_intent_ids - covered_flat_ids

    if uncovered_flat:
        print("\nâš ï¸ 2æ®µéšç›®ã®æŠ½è±¡åŒ–ï¼ˆflattenï¼‰ãŒå…¨ã¦ã®å€‹åˆ¥æ„å›³ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã›ã‚“")
        print(f"   ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„å€‹åˆ¥æ„å›³ID: {sorted(uncovered_flat)}")
        print(
            f"   Total: {len(all_individual_intent_ids)}, Covered: {len(covered_flat_ids)}, Uncovered: {len(uncovered_flat)}"
        )

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    duplicate_flat_ids = []
    for ultra_intent in ultra_intents:
        for intent_id in ultra_intent["covered_intent_ids_flat"]:
            duplicate_flat_ids.append(
                (intent_id["cluster_id"], intent_id["intent_index"])
            )

    if len(duplicate_flat_ids) != len(set(duplicate_flat_ids)):
        dup_set = [x for x in duplicate_flat_ids if duplicate_flat_ids.count(x) > 1]
        print(
            f"\nâš ï¸ 2æ®µéšç›®ã®æŠ½è±¡åŒ–ï¼ˆflattenï¼‰ã§é‡è¤‡ã™ã‚‹å€‹åˆ¥æ„å›³IDãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {set(dup_set)}"
        )

    # æœ€çµ‚çµæœã‚’ä¿å­˜
    output_file = CROSS_CLUSTER_DIR / "ultra_intents.json"
    ultra_result = {
        "generated_at": datetime.now().isoformat(),
        "total_super_intents": len(super_intents),
        "total_meta_intents": len(meta_intents),
        "total_individual_intents": total_individual_intents,
        "ultra_intents": ultra_intents,
        "super_intents": super_intents,
        "validation": {
            "super_level": {
                "total_super_intents": len(super_intents),
                "covered_super_intents": len(covered_indices),
                "uncovered_super_intents": len(uncovered),
                "uncovered_super_indices": sorted(uncovered) if uncovered else [],
                "has_duplicates": len(duplicate_check) != len(set(duplicate_check)),
            },
            "individual_level": {
                "total_individual_intents": len(all_individual_intent_ids),
                "covered_individual_intents": len(covered_flat_ids),
                "uncovered_individual_intents": len(uncovered_flat),
                "uncovered_individual_ids": [
                    {"cluster_id": cid, "intent_index": idx}
                    for cid, idx in sorted(uncovered_flat)
                ],
                "has_duplicates": len(duplicate_flat_ids)
                != len(set(duplicate_flat_ids)),
            },
        },
    }

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(ultra_result, f, ensure_ascii=False, indent=2)

    # è©³ç´°å±•é–‹ç‰ˆã‚‚ç”Ÿæˆï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ã¯å¤‰æ›´ã—ãªã„ï¼‰
    enrich_and_save_ultra_intents(ultra_result, output_file.parent)

    return ultra_intents


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹
    parser = argparse.ArgumentParser(
        description="ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®æ„å›³æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§Gemini APIã«ã‚ˆã‚‹æ„å›³æŠ½å‡ºã‚’å®Ÿè¡Œ"
    )
    parser.add_argument(
        "--gemini",
        action="store_true",
        help="Gemini APIã§æ„å›³æŠ½å‡ºã‚’å®Ÿè¡Œã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨HTMLã‚’ç”Ÿæˆ",
    )
    parser.add_argument(
        "--cluster",
        type=int,
        help="ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚¿IDã®ã¿å‡¦ç†ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯å…¨ã‚¯ãƒ©ã‚¹ã‚¿ï¼‰",
    )
    parser.add_argument(
        "--save-raw",
        action="store_true",
        help="Gemini APIã®ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰",
    )
    parser.add_argument(
        "--aggregate",
        action="store_true",
        help="æŠ½å‡ºã—ãŸæ„å›³ã‹ã‚‰ä¸Šä½æ„å›³ã‚’ç”Ÿæˆï¼ˆ--gemini ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ä½µç”¨ï¼‰",
    )
    parser.add_argument(
        "--aggregate-all",
        action="store_true",
        help="å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸Šä½æ„å›³ã‹ã‚‰ã•ã‚‰ã«ä¸Šä½ã®æ„å›³ã‚’ç”Ÿæˆï¼ˆ--gemini --aggregate ã¨ä½µç”¨ã€--clusteræŒ‡å®šæ™‚ã¯ç„¡åŠ¹ï¼‰",
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=5,
        help="ä¸¦åˆ—å®Ÿè¡Œã®æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰",
    )
    args = parser.parse_args()

    print("=" * 60)
    print("æ„å›³æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ")
    if args.gemini:
        print(f"+ Gemini API ã§æ„å›³æŠ½å‡ºã‚’å®Ÿè¡Œï¼ˆä¸¦åˆ—æ•°: {args.max_workers}ï¼‰")
    if args.aggregate:
        if not args.gemini:
            print(
                "âŒ ã‚¨ãƒ©ãƒ¼: --aggregate ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ --gemini ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ä½µç”¨ã—ã¦ãã ã•ã„"
            )
            return
        print("+ ä¸Šä½æ„å›³ã‚’æŠ½å‡º")
    if args.aggregate_all:
        if not args.gemini or not args.aggregate:
            print(
                "âŒ ã‚¨ãƒ©ãƒ¼: --aggregate-all ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ --gemini --aggregate ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ä½µç”¨ã—ã¦ãã ã•ã„"
            )
            return
        if args.cluster is not None:
            print(
                "âŒ ã‚¨ãƒ©ãƒ¼: --aggregate-all ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ --cluster ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ä½µç”¨ã§ãã¾ã›ã‚“ï¼ˆå…¨ã‚¯ãƒ©ã‚¹ã‚¿å‡¦ç†ãŒå¿…è¦ï¼‰"
            )
            return
        print("+ ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³ã‚’æŠ½å‡º")
    if args.cluster is not None:
        print(f"+ ã‚¯ãƒ©ã‚¹ã‚¿ {args.cluster} ã®ã¿å‡¦ç†")
    if args.save_raw:
        print("+ ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜")
    print("=" * 60)

    # Gemini API ã®åˆæœŸåŒ–ï¼ˆ--gemini ã‚ªãƒ—ã‚·ãƒ§ãƒ³æŒ‡å®šæ™‚ï¼‰
    if args.gemini:
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            print("âŒ ã‚¨ãƒ©ãƒ¼: GEMINI_API_KEY ãŒ .env ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        gemini_client.configure(api_key=api_key)
        print("âœ“ Gemini API ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸï¼ˆlitellm + diskcacheã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹ï¼‰")

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
    print("\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
    try:
        template = load_template()
        print(f"âœ“ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {TEMPLATE_FILE}")
    except FileNotFoundError as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # æ„å›³ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ï¼ˆ--aggregate ã¾ãŸã¯ --aggregate-all ã‚ªãƒ—ã‚·ãƒ§ãƒ³æŒ‡å®šæ™‚ï¼‰
    grouping_template = None
    if args.aggregate or args.aggregate_all:
        print("\næ„å›³ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
        try:
            grouping_template = load_grouping_template()
            print(f"âœ“ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {GROUPING_TEMPLATE_FILE}")
        except FileNotFoundError as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’èª­ã¿è¾¼ã¿ä¸­...")
    df = load_clustered_messages()
    print(f"âœ“ {len(df)}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
    print("\nãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰ä¸­...")
    message_metadata = build_message_metadata(df)
    print(f"âœ“ {len(message_metadata)}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸ")

    # ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    cluster_ids = sorted(df["cluster"].unique())

    # ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã‚¿ã®ã¿å‡¦ç†ã™ã‚‹å ´åˆã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if args.cluster is not None:
        if args.cluster not in cluster_ids:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: ã‚¯ãƒ©ã‚¹ã‚¿ {args.cluster} ã¯å­˜åœ¨ã—ã¾ã›ã‚“")
            print(f"åˆ©ç”¨å¯èƒ½ãªã‚¯ãƒ©ã‚¹ã‚¿ID: {cluster_ids}")
            return
        cluster_ids = [args.cluster]

    print(f"\n{len(cluster_ids)}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«å¯¾ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã™")

    # æ—¢å­˜ã®çµæœã‚’èª­ã¿è¾¼ã¿ï¼ˆéƒ¨åˆ†æ›´æ–°ã®å ´åˆï¼‰
    all_prompts = []
    # if args.cluster is not None and (OUTPUT_DIR / "generation_summary.json").exists():
    #     # æ—¢å­˜ã®ã‚µãƒãƒªãƒ¼ã‹ã‚‰ä»–ã®ã‚¯ãƒ©ã‚¹ã‚¿ã®æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
    #     with open(OUTPUT_DIR / "generation_summary.json", 'r', encoding='utf-8') as f:
    #         existing_summary = json.load(f)
    #     # æ—¢å­˜ã®æŠ½å‡ºçµæœã‚‚èª­ã¿è¾¼ã¿ï¼ˆHTMLå†ç”Ÿæˆã®ãŸã‚ï¼‰
    #     # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ã€æŒ‡å®šã‚¯ãƒ©ã‚¹ã‚¿ã®ã¿å†ç”Ÿæˆ

    # ä¸¦åˆ—åŒ–ã™ã‚‹å ´åˆã®å‡¦ç†é–¢æ•°ã‚’å®šç¾©
    def process_cluster(cluster_id: int) -> Dict:
        """1ã¤ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’å‡¦ç†ã™ã‚‹é–¢æ•°ï¼ˆä¸¦åˆ—å®Ÿè¡Œç”¨ï¼‰"""
        cluster_df = df[df["cluster"] == cluster_id]
        prompt_info = generate_cluster_prompt(cluster_id, cluster_df, template)

        # Gemini APIã§æ„å›³æŠ½å‡ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³æŒ‡å®šæ™‚ï¼‰
        if args.gemini:
            intents = call_gemini_api_with_postprocess(
                prompt_info["prompt"],
                cluster_id,
                message_metadata,
                save_raw=args.save_raw,
            )
            prompt_info["extracted_intents"] = intents

            # ä¸Šä½æ„å›³æŠ½å‡ºï¼ˆ--aggregate ã‚ªãƒ—ã‚·ãƒ§ãƒ³æŒ‡å®šæ™‚ï¼‰
            if args.aggregate and intents and grouping_template:
                meta_intents = aggregate_intents_with_gemini(
                    intents, cluster_id, grouping_template, save_raw=args.save_raw
                )
                prompt_info["meta_intents"] = meta_intents

        # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        output_file = OUTPUT_DIR / f"cluster_{cluster_id:02d}_prompt.md"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(prompt_info["prompt"])

        return prompt_info

    # Gemini APIä½¿ç”¨æ™‚ã¯ä¸¦åˆ—å®Ÿè¡Œã€ãã‚Œä»¥å¤–ã¯é€æ¬¡å®Ÿè¡Œ
    if args.gemini:
        progress_desc = "Gemini API ã§æ„å›³æŠ½å‡ºä¸­"
        all_prompts = gemini_client.parallel_execute(
            cluster_ids,
            process_cluster,
            max_workers=args.max_workers,
            desc=progress_desc,
            unit="cluster",
        )
    else:
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã®ã¿ã®å ´åˆã¯é€æ¬¡å®Ÿè¡Œï¼ˆé«˜é€Ÿãªã®ã§ä¸¦åˆ—åŒ–ä¸è¦ï¼‰
        progress_desc = "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆä¸­"
        all_prompts = []
        for cluster_id in tqdm(cluster_ids, desc=progress_desc, unit="cluster"):
            prompt_info = process_cluster(cluster_id)
            all_prompts.append(prompt_info)

    # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’ä¿å­˜
    summary = {
        "generated_at": datetime.now().isoformat(),
        "total_clusters": int(len(cluster_ids)),
        "total_messages": int(len(df)),
        "clusters": [
            {
                "cluster_id": int(p["cluster_id"]),
                "message_count": int(p["message_count"]),
                "prompt_file": f"cluster_{p['cluster_id']:02d}_prompt.md",
            }
            for p in all_prompts
        ],
    }

    summary_file = OUTPUT_DIR / "generation_summary.json"
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’ä¿å­˜: {summary_file}")

    # ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³æŠ½å‡ºï¼ˆ--aggregate-all ã‚ªãƒ—ã‚·ãƒ§ãƒ³æŒ‡å®šæ™‚ï¼‰
    if args.aggregate_all and grouping_template:
        print("\n" + "=" * 60)
        print("ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³æŠ½å‡º")
        print("=" * 60)

        # å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸Šä½æ„å›³ã‚’åé›†
        print("\nå…¨ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸Šä½æ„å›³ã‚’åé›†ä¸­...")
        all_meta_intents, stats = collect_all_meta_intents(cluster_ids)
        print(f"âœ“ {len(all_meta_intents)}ä»¶ã®ä¸Šä½æ„å›³ã‚’åé›†ã—ã¾ã—ãŸ")
        print(f"âœ“ {stats['total_individual_intents']}ä»¶ã®å€‹åˆ¥æ„å›³ï¼ˆå…¨ã‚¯ãƒ©ã‚¹ã‚¿åˆè¨ˆï¼‰")

        # ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³æŠ½å‡º
        print("\nã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³ã‚’æŠ½å‡ºä¸­...")
        super_intents, meta_intents, total_individual_intents = (
            aggregate_cross_cluster_intents(
                all_meta_intents,
                grouping_template,
                stats["total_individual_intents"],
                save_raw=args.save_raw,
            )
        )

        if super_intents:
            print(f"âœ“ {len(super_intents)}ä»¶ã®ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
            for i, super_intent in enumerate(super_intents, 1):
                covered_count = len(super_intent.get("covered_meta_intent_indices", []))
                print(
                    f"  {i}. {super_intent.get('super_intent', 'ï¼ˆæœªå®šç¾©ï¼‰')} ({covered_count}ä»¶ã®meta_intentã‚’ã‚«ãƒãƒ¼)"
                )

            # 2æ®µéšç›®ã®æŠ½è±¡åŒ–ï¼ˆsuper_intentsãŒ50ä»¶ä»¥ä¸Šã®å ´åˆï¼‰
            ultra_intents = aggregate_super_intents_recursively(
                super_intents,
                meta_intents,
                total_individual_intents,
                grouping_template,
                save_raw=args.save_raw,
            )

            if ultra_intents:
                print("\n" + "=" * 60)
                print("æœ€çµ‚ä¸Šä½æ„å›³æŠ½å‡ºçµæœï¼ˆultra_intentsï¼‰")
                print("=" * 60)
                print(
                    f"\nâœ“ {len(ultra_intents)}ä»¶ã®æœ€çµ‚ä¸Šä½æ„å›³ï¼ˆultra_intentsï¼‰ã‚’æŠ½å‡ºã—ã¾ã—ãŸ\n"
                )
                for i, ultra_intent in enumerate(ultra_intents, 1):
                    covered_super_count = len(
                        ultra_intent.get("covered_super_intent_indices", [])
                    )
                    covered_intent_count = len(
                        ultra_intent.get("covered_intent_ids_flat", [])
                    )
                    print(f"ã€Ultra Intent {i}ã€‘")
                    print(f"  æ„å›³: {ultra_intent.get('ultra_intent', 'ï¼ˆæœªå®šç¾©ï¼‰')}")
                    if ultra_intent.get("objective_facts"):
                        print(f"  å®¢è¦³çš„äº‹å®Ÿ: {ultra_intent['objective_facts']}")
                    if ultra_intent.get("context"):
                        print(f"  èƒŒæ™¯: {ultra_intent['context']}")
                    print(
                        f"  ã‚«ãƒãƒ¼ç¯„å›²: {covered_super_count}ä»¶ã®super_intent / {covered_intent_count}ä»¶ã®å€‹åˆ¥æ„å›³"
                    )
                    if ultra_intent.get("source_full_paths"):
                        paths = ", ".join(ultra_intent["source_full_paths"][:3])
                        if len(ultra_intent["source_full_paths"]) > 3:
                            paths += (
                                f" ä»–{len(ultra_intent['source_full_paths']) - 3}ä»¶"
                            )
                        print(f"  ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {paths}")
                    print(
                        f"  ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {ultra_intent.get('aggregate_status', 'unknown')}"
                    )
                    print()
        else:
            print("âŒ ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")

    # ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã®HTMLã‚’ç”Ÿæˆ
    if args.gemini:
        # GeminiæŠ½å‡ºçµæœã®ãƒ¬ãƒ“ãƒ¥ãƒ¼HTML
        generate_intent_review_html(all_prompts, include_meta_intents=args.aggregate)
        print("\n" + "=" * 60)
        print("âœ… æ„å›³æŠ½å‡ºå®Œäº†ï¼")
        print("=" * 60)
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {OUTPUT_DIR}")
        print(f"ğŸ“„ ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨HTML: {OUTPUT_DIR / 'intent_review.html'}")
        print(f"ğŸ“„ å¾Œå‡¦ç†æ¸ˆã¿JSON: {PROCESSED_DIR}/")
        if args.aggregate:
            print(f"ğŸ“„ ä¸Šä½æ„å›³JSON: {AGGREGATED_DIR}/")
        if args.aggregate_all:
            print(
                f"ğŸ“„ ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³JSON: {CROSS_CLUSTER_DIR}/super_intents.json"
            )
            # super_intents_enriched.json ã®å­˜åœ¨ã‚’ãƒã‚§ãƒƒã‚¯
            super_enriched = CROSS_CLUSTER_DIR / "super_intents_enriched.json"
            if super_enriched.exists():
                print(f"ğŸ“„ ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³JSONï¼ˆè©³ç´°å±•é–‹ç‰ˆï¼‰: {super_enriched}")

            # ultra_intents.json ã®å­˜åœ¨ã‚’ãƒã‚§ãƒƒã‚¯
            ultra_file = CROSS_CLUSTER_DIR / "ultra_intents.json"
            if ultra_file.exists():
                print(f"ğŸ“„ æœ€çµ‚ä¸Šä½æ„å›³JSONï¼ˆ2æ®µéšæŠ½è±¡åŒ–ï¼‰: {ultra_file}")
                enriched_file = CROSS_CLUSTER_DIR / "ultra_intents_enriched.json"
                if enriched_file.exists():
                    print(f"ğŸ“„ æœ€çµ‚ä¸Šä½æ„å›³JSONï¼ˆè©³ç´°å±•é–‹ç‰ˆï¼‰: {enriched_file}")
        print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print(f"  1. {OUTPUT_DIR}/intent_review.html ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã")
        print("  2. æŠ½å‡ºã•ã‚ŒãŸæ„å›³ã‚’ç¢ºèªãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        if args.aggregate_all:
            print("  3. ã‚¯ãƒ©ã‚¹ã‚¿æ¨ªæ–­ä¸Šä½æ„å›³ã®éšå±¤æ§‹é€ ã‚’ç¢ºèª")
            print(f"  4. {CROSS_CLUSTER_DIR}/super_intents.json ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª")
            super_enriched = CROSS_CLUSTER_DIR / "super_intents_enriched.json"
            if super_enriched.exists():
                print(f"  5. {super_enriched} ã®è©³ç´°å±•é–‹ç‰ˆï¼ˆå€‹åˆ¥æ„å›³è©³ç´°å«ã‚€ï¼‰ã‚’ç¢ºèª")
            ultra_file = CROSS_CLUSTER_DIR / "ultra_intents.json"
            enriched_file = CROSS_CLUSTER_DIR / "ultra_intents_enriched.json"
            if ultra_file.exists():
                print(f"  6. {ultra_file} ã®æœ€çµ‚ä¸Šä½æ„å›³ï¼ˆ2æ®µéšæŠ½è±¡åŒ–ï¼‰ã‚’ç¢ºèª")
            if enriched_file.exists():
                print(f"  7. {enriched_file} ã®è©³ç´°å±•é–‹ç‰ˆï¼ˆå€‹åˆ¥æ„å›³è©³ç´°å«ã‚€ï¼‰ã‚’ç¢ºèª")
        elif args.aggregate:
            print("  3. ä¸Šä½æ„å›³ã¨å€‹åˆ¥æ„å›³ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ç¢ºèª")
            print(f"  4. {AGGREGATED_DIR}/ ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª")
        else:
            print(f"  3. {PROCESSED_DIR}/ ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª")
    else:
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¸€è¦§ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹HTML
        generate_review_index(all_prompts)
        print("\n" + "=" * 60)
        print("âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå®Œäº†ï¼")
        print("=" * 60)
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {OUTPUT_DIR}")
        print(f"ğŸ“„ ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {OUTPUT_DIR / 'index.html'}")
        print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print(f"  1. {OUTPUT_DIR}/index.html ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã")
        print("  2. å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        print("  3. --gemini ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æ„å›³æŠ½å‡ºã‚’å®Ÿè¡Œ")


def generate_intent_review_html(
    all_prompts: List[Dict], include_meta_intents: bool = False
):
    """GeminiæŠ½å‡ºçµæœã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨HTMLã‚’ç”Ÿæˆ"""
    html_parts = [
        "<!DOCTYPE html>",
        "<html lang='ja'>",
        "<head>",
        "  <meta charset='UTF-8'>",
        "  <meta name='viewport' content='width=device-width, initial-scale=1.0'>",
        "  <title>æ„å›³æŠ½å‡ºçµæœ - ãƒ¬ãƒ“ãƒ¥ãƒ¼</title>",
        "  <style>",
        "    body { font-family: 'Hiragino Sans', sans-serif; margin: 20px; background: #f5f5f5; }",
        "    .container { max-width: 1400px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }",
        "    h1 { color: #333; border-bottom: 3px solid #4CAF50; padding-bottom: 10px; }",
        "    h2 { color: #333; border-bottom: 2px solid #FF9800; padding-bottom: 8px; margin-top: 20px; }",
        "    .summary { background: #e8f5e9; padding: 15px; border-radius: 5px; margin: 20px 0; }",
        "    .cluster-section { margin: 30px 0; padding: 20px; background: #fafafa; border-radius: 8px; }",
        "    .cluster-header { background: #4CAF50; color: white; padding: 15px; border-radius: 5px; margin-bottom: 15px; }",
        "    .cluster-title { font-size: 1.3em; font-weight: bold; }",
        "    .cluster-meta { margin-top: 5px; font-size: 0.9em; opacity: 0.9; }",
        "    .meta-intents-container { margin-top: 20px; }",
        "    .meta-intent-card { background: #FFF8E1; border: 2px solid #FF9800; padding: 18px; margin: 15px 0; border-radius: 8px; }",
        "    .meta-intent-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 12px; }",
        "    .meta-intent-title { font-size: 1.2em; font-weight: bold; color: #E65100; }",
        "    .meta-intent-field { margin: 10px 0; }",
        "    .covered-intents { background: #FFF3E0; padding: 10px; margin-top: 10px; border-radius: 5px; }",
        "    .covered-intent-link { display: inline-block; background: #FFE0B2; padding: 4px 10px; margin: 3px; border-radius: 4px; font-size: 0.9em; }",
        "    .intents-container { margin-top: 15px; }",
        "    .intent-card { background: white; border: 1px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px; border-left: 4px solid #2196F3; }",
        "    .intent-card.covered { opacity: 0.7; }",
        "    .intent-index { display: inline-block; background: #9E9E9E; color: white; padding: 2px 8px; border-radius: 10px; font-size: 0.8em; margin-right: 8px; }",
        "    .intent-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px; }",
        "    .intent-description { font-size: 1.1em; font-weight: bold; color: #333; }",
        "    .intent-status { padding: 4px 12px; border-radius: 12px; font-size: 0.85em; font-weight: bold; }",
        "    .status-idea { background: #E3F2FD; color: #1976D2; }",
        "    .status-todo { background: #FFF3E0; color: #F57C00; }",
        "    .status-doing { background: #F3E5F5; color: #7B1FA2; }",
        "    .status-done { background: #E8F5E9; color: #388E3C; }",
        "    .intent-field { margin: 8px 0; }",
        "    .field-label { font-weight: bold; color: #666; font-size: 0.9em; }",
        "    .field-value { margin-top: 3px; color: #333; }",
        "    .message-ids { display: flex; flex-wrap: wrap; gap: 5px; margin-top: 5px; }",
        "    .message-id-tag { background: #E0E0E0; padding: 3px 8px; border-radius: 3px; font-size: 0.85em; font-family: monospace; }",
        "    .error-message { background: #FFEBEE; color: #C62828; padding: 15px; border-radius: 5px; border-left: 4px solid #C62828; }",
        "    .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0; }",
        "    .stat-card { background: white; border: 1px solid #ddd; padding: 15px; border-radius: 5px; text-align: center; }",
        "    .stat-value { font-size: 2em; font-weight: bold; color: #4CAF50; }",
        "    .stat-label { color: #666; font-size: 0.9em; margin-top: 5px; }",
        "  </style>",
        "</head>",
        "<body>",
        "  <div class='container'>",
        "    <h1>ğŸ¯ æ„å›³æŠ½å‡ºçµæœ - ãƒ¬ãƒ“ãƒ¥ãƒ¼</h1>",
    ]

    # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
    total_clusters = len(all_prompts)
    total_intents = sum(
        len(p.get("extracted_intents", []))
        for p in all_prompts
        if p.get("extracted_intents")
    )
    failed_clusters = sum(1 for p in all_prompts if not p.get("extracted_intents"))
    success_clusters = total_clusters - failed_clusters

    html_parts.extend(
        [
            "    <div class='summary'>",
            f"      <strong>ç”Ÿæˆæ—¥æ™‚:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br>",
            "      <strong>ãƒ¢ãƒ‡ãƒ«:</strong> Gemini 2.5 Flash",
            "    </div>",
            "    <div class='stats'>",
            f"      <div class='stat-card'><div class='stat-value'>{total_clusters}</div><div class='stat-label'>ã‚¯ãƒ©ã‚¹ã‚¿æ•°</div></div>",
            f"      <div class='stat-card'><div class='stat-value'>{total_intents}</div><div class='stat-label'>æŠ½å‡ºã•ã‚ŒãŸæ„å›³</div></div>",
            f"      <div class='stat-card'><div class='stat-value'>{success_clusters}</div><div class='stat-label'>æˆåŠŸ</div></div>",
            f"      <div class='stat-card'><div class='stat-value'>{failed_clusters}</div><div class='stat-label'>å¤±æ•—</div></div>",
            "    </div>",
        ]
    )

    # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®çµæœã‚’è¡¨ç¤º
    for prompt_info in all_prompts:
        cluster_id = prompt_info["cluster_id"]
        message_count = prompt_info["message_count"]
        intents = prompt_info.get("extracted_intents")
        meta_intents = prompt_info.get("meta_intents")

        html_parts.extend(
            [
                "    <div class='cluster-section'>",
                "      <div class='cluster-header'>",
                f"        <div class='cluster-title'>ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}</div>",
                f"        <div class='cluster-meta'>{message_count}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸</div>",
                "      </div>",
            ]
        )

        # ä¸Šä½æ„å›³ã‚’è¡¨ç¤ºï¼ˆ--aggregate æŒ‡å®šæ™‚ï¼‰
        if include_meta_intents and meta_intents:
            html_parts.append("      <h2>ğŸ¯ ä¸Šä½æ„å›³ï¼ˆMeta Intentsï¼‰</h2>")
            html_parts.append("      <div class='meta-intents-container'>")

            # ã©ã®æ„å›³ãŒä¸Šä½æ„å›³ã§ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã‚‹ã‹è¿½è·¡
            covered_indices = set()
            for meta_intent in meta_intents:
                covered_indices.update(meta_intent.get("covered_intent_indices", []))

            for idx, meta_intent in enumerate(meta_intents, 1):
                meta_status = meta_intent.get("aggregate_status", "unknown")
                status_class = f"status-{meta_status}"

                html_parts.extend(
                    [
                        "        <div class='meta-intent-card'>",
                        "          <div class='meta-intent-header'>",
                        f"            <div class='meta-intent-title'>{idx}. {meta_intent.get('meta_intent', 'ï¼ˆæœªå®šç¾©ï¼‰')}</div>",
                        f"            <div class='intent-status {status_class}'>{meta_status}</div>",
                        "          </div>",
                    ]
                )

                # objective_facts
                if meta_intent.get("objective_facts"):
                    html_parts.extend(
                        [
                            "          <div class='meta-intent-field'>",
                            "            <div class='field-label'>å®¢è¦³çš„äº‹å®Ÿ:</div>",
                            f"            <div class='field-value'>{meta_intent['objective_facts']}</div>",
                            "          </div>",
                        ]
                    )

                # context
                if meta_intent.get("context"):
                    html_parts.extend(
                        [
                            "          <div class='meta-intent-field'>",
                            "            <div class='field-label'>èƒŒæ™¯:</div>",
                            f"            <div class='field-value'>{meta_intent['context']}</div>",
                            "          </div>",
                        ]
                    )

                # covered intents
                if meta_intent.get("covered_intent_indices"):
                    html_parts.extend(
                        [
                            "          <div class='meta-intent-field'>",
                            "            <div class='field-label'>å«ã¾ã‚Œã‚‹å€‹åˆ¥æ„å›³:</div>",
                            "            <div class='covered-intents'>",
                        ]
                    )
                    for intent_idx in meta_intent["covered_intent_indices"]:
                        html_parts.append(
                            f"              <span class='covered-intent-link'>Intent #{intent_idx}</span>"
                        )
                    html_parts.extend(
                        [
                            "            </div>",
                            "          </div>",
                        ]
                    )

                html_parts.append("        </div>")

            html_parts.append("      </div>")
        else:
            covered_indices = set()

        # å€‹åˆ¥æ„å›³ã‚’è¡¨ç¤º
        if intents:
            html_parts.append(
                "      <h2>ğŸ“ å€‹åˆ¥æ„å›³ï¼ˆIndividual Intentsï¼‰</h2>"
                if include_meta_intents and meta_intents
                else ""
            )
            html_parts.append("      <div class='intents-container'>")
            for i, intent in enumerate(intents):
                status = intent.get("status", "unknown")
                status_class = f"status-{status}"

                # ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã‚‹æ„å›³ã¯è–„ãè¡¨ç¤º
                covered_class = " covered" if i in covered_indices else ""

                # æ„å›³ã®èª¬æ˜æ–‡ã‚’æŸ”è»Ÿã«å–å¾—ï¼ˆdescription, intent, ãã®ä»–ã®é †ã§æ¢ã™ï¼‰
                description = (
                    intent.get("description") or intent.get("intent") or "ï¼ˆèª¬æ˜ãªã—ï¼‰"
                )

                html_parts.extend(
                    [
                        f"        <div class='intent-card{covered_class}'>",
                        "          <div class='intent-header'>",
                        f"            <div class='intent-description'><span class='intent-index'>#{i}</span>{description}</div>",
                        f"            <div class='intent-status {status_class}'>{status}</div>",
                        "          </div>",
                    ]
                )

                # ç‰¹åˆ¥ãªã‚­ãƒ¼ã‚’é™¤å¤–ã—ã¦ã€æ®‹ã‚Šã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‹•çš„ã«è¡¨ç¤º
                special_keys = {"description", "intent", "status", "source_message_ids"}

                # æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°
                label_map = {
                    "target": "å¯¾è±¡",
                    "motivation": "å‹•æ©Ÿ",
                    "why": "ç†ç”±",
                    "objective_facts": "å®¢è¦³çš„äº‹å®Ÿ",
                }

                for key, value in intent.items():
                    if key in special_keys:
                        continue
                    if key == "source_message_ids":
                        continue
                    if value is None or value == "":
                        continue

                    label = label_map.get(key, key)
                    html_parts.extend(
                        [
                            "          <div class='intent-field'>",
                            f"            <div class='field-label'>{label}:</div>",
                            f"            <div class='field-value'>{value}</div>",
                            "          </div>",
                        ]
                    )

                # source_message_idsï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
                if intent.get("source_message_ids"):
                    html_parts.extend(
                        [
                            "          <div class='intent-field'>",
                            "            <div class='field-label'>ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ID:</div>",
                            "            <div class='message-ids'>",
                        ]
                    )
                    for msg_id in intent["source_message_ids"]:
                        html_parts.append(
                            f"              <span class='message-id-tag'>{msg_id}</span>"
                        )
                    html_parts.extend(
                        [
                            "            </div>",
                            "          </div>",
                        ]
                    )

                html_parts.append("        </div>")

            html_parts.append("      </div>")
        else:
            html_parts.append(
                "      <div class='error-message'>âš ï¸ æ„å›³æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ</div>"
            )

        html_parts.append("    </div>")

    html_parts.extend(
        [
            "  </div>",
            "</body>",
            "</html>",
        ]
    )

    output_file = OUTPUT_DIR / "intent_review.html"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("\n".join(html_parts))

    print(f"\nâœ“ æ„å›³æŠ½å‡ºãƒ¬ãƒ“ãƒ¥ãƒ¼HTMLã‚’ç”Ÿæˆ: {output_file}")


def generate_review_index(all_prompts: List[Dict]):
    """ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã®HTMLã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆ"""
    html_parts = [
        "<!DOCTYPE html>",
        "<html lang='ja'>",
        "<head>",
        "  <meta charset='UTF-8'>",
        "  <meta name='viewport' content='width=device-width, initial-scale=1.0'>",
        "  <title>æ„å›³æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ - ãƒ¬ãƒ“ãƒ¥ãƒ¼</title>",
        "  <style>",
        "    body { font-family: 'Hiragino Sans', sans-serif; margin: 20px; background: #f5f5f5; }",
        "    .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }",
        "    h1 { color: #333; border-bottom: 3px solid #4CAF50; padding-bottom: 10px; }",
        "    .summary { background: #e8f5e9; padding: 15px; border-radius: 5px; margin: 20px 0; }",
        "    .cluster-list { margin-top: 30px; }",
        "    .cluster-item { background: #fff; border: 1px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px; }",
        "    .cluster-item:hover { background: #f9f9f9; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }",
        "    .cluster-header { display: flex; justify-content: space-between; align-items: center; }",
        "    .cluster-id { font-size: 1.2em; font-weight: bold; color: #4CAF50; }",
        "    .message-count { color: #666; font-size: 0.9em; }",
        "    .preview { margin-top: 10px; padding: 10px; background: #f5f5f5; border-left: 3px solid #4CAF50; font-size: 0.85em; max-height: 100px; overflow: hidden; }",
        "    .actions { margin-top: 10px; }",
        "    .btn { display: inline-block; padding: 8px 16px; background: #4CAF50; color: white; text-decoration: none; border-radius: 4px; margin-right: 10px; }",
        "    .btn:hover { background: #45a049; }",
        "    .btn-secondary { background: #2196F3; }",
        "    .btn-secondary:hover { background: #0b7dda; }",
        "  </style>",
        "</head>",
        "<body>",
        "  <div class='container'>",
        "    <h1>ğŸ¯ æ„å›³æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ - ãƒ¬ãƒ“ãƒ¥ãƒ¼</h1>",
        "    <div class='summary'>",
        f"      <strong>ç”Ÿæˆæ—¥æ™‚:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br>",
        f"      <strong>ã‚¯ãƒ©ã‚¹ã‚¿æ•°:</strong> {len(all_prompts)}<br>",
        f"      <strong>ç·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°:</strong> {sum(p['message_count'] for p in all_prompts)}",
        "    </div>",
        "    <div class='cluster-list'>",
    ]

    for prompt_info in all_prompts:
        cluster_id = prompt_info["cluster_id"]
        message_count = prompt_info["message_count"]
        filename = f"cluster_{cluster_id:02d}_prompt.md"

        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã«æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
        first_msg = prompt_info["messages"][0] if prompt_info["messages"] else None
        preview = first_msg["content"][:200] + "..." if first_msg else ""

        html_parts.extend(
            [
                "      <div class='cluster-item'>",
                "        <div class='cluster-header'>",
                f"          <div class='cluster-id'>ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}</div>",
                f"          <div class='message-count'>{message_count}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸</div>",
                "        </div>",
                f"        <div class='preview'>{preview}</div>",
                "        <div class='actions'>",
                f"          <a href='{filename}' class='btn' target='_blank'>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é–‹ã</a>",
                f"          <button class='btn btn-secondary' onclick='copyToClipboard(\"{filename}\")'>ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã«ã‚³ãƒ”ãƒ¼</button>",
                "        </div>",
                "      </div>",
            ]
        )

    html_parts.extend(
        [
            "    </div>",
            "  </div>",
            "  <script>",
            "    async function copyToClipboard(filename) {",
            "      try {",
            "        const response = await fetch(filename);",
            "        const text = await response.text();",
            "        await navigator.clipboard.writeText(text);",
            "        alert('ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸï¼');",
            "      } catch (err) {",
            "        alert('ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ã—ã¾ã—ãŸ: ' + err);",
            "      }",
            "    }",
            "  </script>",
            "</body>",
            "</html>",
        ]
    )

    index_file = OUTPUT_DIR / "index.html"
    with open(index_file, "w", encoding="utf-8") as f:
        f.write("\n".join(html_parts))

    print(f"\nâœ“ ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆ: {index_file}")


if __name__ == "__main__":
    main()
